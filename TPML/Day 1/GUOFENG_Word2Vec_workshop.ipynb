{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_workshop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_89yP2qzEui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc8579c4-717f-4d93-8636-f17f342644e1"
      },
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "!pip install plot_keras_history\n",
        "!pip install seaborn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.31.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.2.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: plot_keras_history in /usr/local/lib/python3.6/dist-packages (1.1.23)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from plot_keras_history) (1.0.5)\n",
            "Requirement already satisfied: sanitize-ml-labels in /usr/local/lib/python3.6/dist-packages (from plot_keras_history) (1.0.12)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from plot_keras_history) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from plot_keras_history) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->plot_keras_history) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->plot_keras_history) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->plot_keras_history) (2.8.1)\n",
            "Requirement already satisfied: compress-json in /usr/local/lib/python3.6/dist-packages (from sanitize-ml-labels->plot_keras_history) (1.0.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot_keras_history) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot_keras_history) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot_keras_history) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->plot_keras_history) (1.15.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.18.5)\n",
            "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.0.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.2->seaborn) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeiZ_Cqvzoqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda, Reshape\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import dot\n",
        "from tensorflow.keras.activations import relu\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "import gensim\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZazoZFH0BZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "8de00075-2fcf-4692-eacc-b7ddce2e86da"
      },
      "source": [
        "# using nltk tokenizer.  \n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co69rL7jzrh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "87f27395-e114-4240-8eff-e326fa66c62d"
      },
      "source": [
        "#Data Preparation \n",
        "\n",
        "AlotOftext = \"\"\"Language users never choose words randomly, and language is essentially\n",
        "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
        "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
        "the null hypothesis will never be true. Moreover, where there is enough\n",
        "data, we shall (almost) always be able to establish that it is not true. In\n",
        "corpus studies, we frequently do have enough data, so the fact that a relation \n",
        "between two phenomena is demonstrably non-random, does not support the inference \n",
        "that it is not arbitrary. We present experimental evidence\n",
        "of how arbitrary associations between word frequencies and corpora are\n",
        "systematically non-random. We review literature in which hypothesis testing \n",
        "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
        "\n",
        "\n",
        "\n",
        "#Tokenize text\n",
        "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(AlotOftext)]\n",
        "\n",
        "#Create Vocab as a Dictionary\n",
        "vocab = Dictionary(tokenized_text)\n",
        "print(dict(vocab.items()))\n",
        "\n",
        "print(vocab.token2id['corpora'])\n",
        "print(vocab[2])\n",
        "sent0 = tokenized_text[0]\n",
        "print(vocab.doc2idx(sent0))\n",
        "\n",
        "vocab.add_documents([['PAD']])\n",
        "dict(vocab.items())\n",
        "print(vocab.token2id['PAD'])\n",
        "\n",
        "corpusByWordID = list()\n",
        "for sent in  tokenized_text:\n",
        "    corpusByWordID.append(vocab.doc2idx(sent))\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embed_size = 100\n",
        "window_size = 2 # context window size\n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(vocab.items())[:10])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: ',', 1: '.', 2: 'and', 3: 'choose', 4: 'essentially', 5: 'is', 6: 'language', 7: 'never', 8: 'non-random', 9: 'randomly', 10: 'users', 11: 'words', 12: 'a', 13: 'hypothesis', 14: 'null', 15: 'posits', 16: 'randomness', 17: 'statistical', 18: 'testing', 19: 'uses', 20: 'which', 21: 'at', 22: 'be', 23: 'corpora', 24: 'hence', 25: 'in', 26: 'linguistic', 27: 'look', 28: 'phenomena', 29: 'the', 30: 'true', 31: 'we', 32: 'when', 33: 'will', 34: '(', 35: ')', 36: 'able', 37: 'almost', 38: 'always', 39: 'data', 40: 'enough', 41: 'establish', 42: 'it', 43: 'moreover', 44: 'not', 45: 'shall', 46: 'that', 47: 'there', 48: 'to', 49: 'where', 50: 'arbitrary', 51: 'between', 52: 'corpus', 53: 'demonstrably', 54: 'do', 55: 'does', 56: 'fact', 57: 'frequently', 58: 'have', 59: 'inference', 60: 'relation', 61: 'so', 62: 'studies', 63: 'support', 64: 'two', 65: 'are', 66: 'associations', 67: 'evidence', 68: 'experimental', 69: 'frequencies', 70: 'how', 71: 'of', 72: 'present', 73: 'systematically', 74: 'word', 75: 'been', 76: 'has', 77: 'led', 78: 'literature', 79: 'misleading', 80: 'often', 81: 'or', 82: 'results', 83: 'review', 84: 'show', 85: 'unhelpful', 86: 'used'}\n",
            "23\n",
            "and\n",
            "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]\n",
            "87\n",
            "Vocabulary Size: 88\n",
            "Vocabulary Sample: [(0, ','), (1, '.'), (2, 'and'), (3, 'choose'), (4, 'essentially'), (5, 'is'), (6, 'language'), (7, 'never'), (8, 'non-random'), (9, 'randomly')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBxqCqAL1BCo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2436
        },
        "outputId": "1e9114b7-9622-4664-deb8-f58a816898d6"
      },
      "source": [
        "# Create CBOW Training data\n",
        "def generate_cbow_context_word_pairs(corpusByID, window_size, vocab_size):\n",
        "    context_length = window_size*2\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    for sent in corpusByID:\n",
        "        sentence_length = len(sent)\n",
        "        for index, word in enumerate(sent):\n",
        "            context_words = []\n",
        "            label_word   = []            \n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "            \n",
        "            context_words.append([sent[i] \n",
        "                                 for i in range(start, end) \n",
        "                                 if 0 <= i < sentence_length \n",
        "                                 and i != index])\n",
        "            label_word.append(word)\n",
        "            if start<0:\n",
        "                x = sequence.pad_sequences(context_words, maxlen=context_length,padding='pre',value=vocab.token2id['PAD'])\n",
        "                y = np_utils.to_categorical(label_word, vocab_size)\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "                continue\n",
        "            if end>=sentence_length:\n",
        "                x = sequence.pad_sequences(context_words, maxlen=context_length,padding='post',value=vocab.token2id['PAD'])\n",
        "                y = np_utils.to_categorical(label_word, vocab_size)\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "                continue\n",
        "            else:\n",
        "                X.append(sequence.pad_sequences(context_words, maxlen=context_length))\n",
        "                Y.append(y)\n",
        "                continue\n",
        "           \n",
        "    return X,Y\n",
        "            \n",
        "# Test this out for some samples\n",
        "\n",
        "\n",
        "X,Y = generate_cbow_context_word_pairs(corpusByWordID, window_size, vocab_size) \n",
        "   \n",
        "for x, y in zip(X,Y):\n",
        "    print('Context (X):', [vocab[w] for w in x[0]], '-> Target (Y):', vocab[np.argwhere(y[0])[0][0]])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context (X): ['PAD', 'PAD', 'users', 'never'] -> Target (Y): language\n",
            "Context (X): ['PAD', 'language', 'never', 'choose'] -> Target (Y): users\n",
            "Context (X): ['language', 'users', 'choose', 'words'] -> Target (Y): users\n",
            "Context (X): ['users', 'never', 'words', 'randomly'] -> Target (Y): users\n",
            "Context (X): ['never', 'choose', 'randomly', ','] -> Target (Y): users\n",
            "Context (X): ['choose', 'words', ',', 'and'] -> Target (Y): users\n",
            "Context (X): ['words', 'randomly', 'and', 'language'] -> Target (Y): users\n",
            "Context (X): ['randomly', ',', 'language', 'is'] -> Target (Y): users\n",
            "Context (X): [',', 'and', 'is', 'essentially'] -> Target (Y): users\n",
            "Context (X): ['and', 'language', 'essentially', 'non-random'] -> Target (Y): users\n",
            "Context (X): ['language', 'is', 'non-random', '.'] -> Target (Y): essentially\n",
            "Context (X): ['is', 'essentially', '.', 'PAD'] -> Target (Y): non-random\n",
            "Context (X): ['essentially', 'non-random', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', 'hypothesis', 'testing'] -> Target (Y): statistical\n",
            "Context (X): ['PAD', 'statistical', 'testing', 'uses'] -> Target (Y): hypothesis\n",
            "Context (X): ['statistical', 'hypothesis', 'uses', 'a'] -> Target (Y): hypothesis\n",
            "Context (X): ['hypothesis', 'testing', 'a', 'null'] -> Target (Y): hypothesis\n",
            "Context (X): ['testing', 'uses', 'null', 'hypothesis'] -> Target (Y): hypothesis\n",
            "Context (X): ['uses', 'a', 'hypothesis', ','] -> Target (Y): hypothesis\n",
            "Context (X): ['a', 'null', ',', 'which'] -> Target (Y): hypothesis\n",
            "Context (X): ['null', 'hypothesis', 'which', 'posits'] -> Target (Y): hypothesis\n",
            "Context (X): ['hypothesis', ',', 'posits', 'randomness'] -> Target (Y): hypothesis\n",
            "Context (X): [',', 'which', 'randomness', '.'] -> Target (Y): posits\n",
            "Context (X): ['which', 'posits', '.', 'PAD'] -> Target (Y): randomness\n",
            "Context (X): ['posits', 'randomness', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', ',', 'when'] -> Target (Y): hence\n",
            "Context (X): ['PAD', 'hence', 'when', 'we'] -> Target (Y): ,\n",
            "Context (X): ['hence', ',', 'we', 'look'] -> Target (Y): ,\n",
            "Context (X): [',', 'when', 'look', 'at'] -> Target (Y): ,\n",
            "Context (X): ['when', 'we', 'at', 'linguistic'] -> Target (Y): ,\n",
            "Context (X): ['we', 'look', 'linguistic', 'phenomena'] -> Target (Y): ,\n",
            "Context (X): ['look', 'at', 'phenomena', 'in'] -> Target (Y): ,\n",
            "Context (X): ['at', 'linguistic', 'in', 'corpora'] -> Target (Y): ,\n",
            "Context (X): ['linguistic', 'phenomena', 'corpora', ','] -> Target (Y): ,\n",
            "Context (X): ['phenomena', 'in', ',', 'the'] -> Target (Y): ,\n",
            "Context (X): ['in', 'corpora', 'the', 'null'] -> Target (Y): ,\n",
            "Context (X): ['corpora', ',', 'null', 'hypothesis'] -> Target (Y): ,\n",
            "Context (X): [',', 'the', 'hypothesis', 'will'] -> Target (Y): ,\n",
            "Context (X): ['the', 'null', 'will', 'never'] -> Target (Y): ,\n",
            "Context (X): ['null', 'hypothesis', 'never', 'be'] -> Target (Y): ,\n",
            "Context (X): ['hypothesis', 'will', 'be', 'true'] -> Target (Y): ,\n",
            "Context (X): ['will', 'never', 'true', '.'] -> Target (Y): be\n",
            "Context (X): ['never', 'be', '.', 'PAD'] -> Target (Y): true\n",
            "Context (X): ['be', 'true', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', ',', 'where'] -> Target (Y): moreover\n",
            "Context (X): ['PAD', 'moreover', 'where', 'there'] -> Target (Y): ,\n",
            "Context (X): ['moreover', ',', 'there', 'is'] -> Target (Y): ,\n",
            "Context (X): [',', 'where', 'is', 'enough'] -> Target (Y): ,\n",
            "Context (X): ['where', 'there', 'enough', 'data'] -> Target (Y): ,\n",
            "Context (X): ['there', 'is', 'data', ','] -> Target (Y): ,\n",
            "Context (X): ['is', 'enough', ',', 'we'] -> Target (Y): ,\n",
            "Context (X): ['enough', 'data', 'we', 'shall'] -> Target (Y): ,\n",
            "Context (X): ['data', ',', 'shall', '('] -> Target (Y): ,\n",
            "Context (X): [',', 'we', '(', 'almost'] -> Target (Y): ,\n",
            "Context (X): ['we', 'shall', 'almost', ')'] -> Target (Y): ,\n",
            "Context (X): ['shall', '(', ')', 'always'] -> Target (Y): ,\n",
            "Context (X): ['(', 'almost', 'always', 'be'] -> Target (Y): ,\n",
            "Context (X): ['almost', ')', 'be', 'able'] -> Target (Y): ,\n",
            "Context (X): [')', 'always', 'able', 'to'] -> Target (Y): ,\n",
            "Context (X): ['always', 'be', 'to', 'establish'] -> Target (Y): ,\n",
            "Context (X): ['be', 'able', 'establish', 'that'] -> Target (Y): ,\n",
            "Context (X): ['able', 'to', 'that', 'it'] -> Target (Y): ,\n",
            "Context (X): ['to', 'establish', 'it', 'is'] -> Target (Y): ,\n",
            "Context (X): ['establish', 'that', 'is', 'not'] -> Target (Y): ,\n",
            "Context (X): ['that', 'it', 'not', 'true'] -> Target (Y): ,\n",
            "Context (X): ['it', 'is', 'true', '.'] -> Target (Y): not\n",
            "Context (X): ['is', 'not', '.', 'PAD'] -> Target (Y): true\n",
            "Context (X): ['not', 'true', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', 'corpus', 'studies'] -> Target (Y): in\n",
            "Context (X): ['PAD', 'in', 'studies', ','] -> Target (Y): corpus\n",
            "Context (X): ['in', 'corpus', ',', 'we'] -> Target (Y): corpus\n",
            "Context (X): ['corpus', 'studies', 'we', 'frequently'] -> Target (Y): corpus\n",
            "Context (X): ['studies', ',', 'frequently', 'do'] -> Target (Y): corpus\n",
            "Context (X): [',', 'we', 'do', 'have'] -> Target (Y): corpus\n",
            "Context (X): ['we', 'frequently', 'have', 'enough'] -> Target (Y): corpus\n",
            "Context (X): ['frequently', 'do', 'enough', 'data'] -> Target (Y): corpus\n",
            "Context (X): ['do', 'have', 'data', ','] -> Target (Y): corpus\n",
            "Context (X): ['have', 'enough', ',', 'so'] -> Target (Y): corpus\n",
            "Context (X): ['enough', 'data', 'so', 'the'] -> Target (Y): corpus\n",
            "Context (X): ['data', ',', 'the', 'fact'] -> Target (Y): corpus\n",
            "Context (X): [',', 'so', 'fact', 'that'] -> Target (Y): corpus\n",
            "Context (X): ['so', 'the', 'that', 'a'] -> Target (Y): corpus\n",
            "Context (X): ['the', 'fact', 'a', 'relation'] -> Target (Y): corpus\n",
            "Context (X): ['fact', 'that', 'relation', 'between'] -> Target (Y): corpus\n",
            "Context (X): ['that', 'a', 'between', 'two'] -> Target (Y): corpus\n",
            "Context (X): ['a', 'relation', 'two', 'phenomena'] -> Target (Y): corpus\n",
            "Context (X): ['relation', 'between', 'phenomena', 'is'] -> Target (Y): corpus\n",
            "Context (X): ['between', 'two', 'is', 'demonstrably'] -> Target (Y): corpus\n",
            "Context (X): ['two', 'phenomena', 'demonstrably', 'non-random'] -> Target (Y): corpus\n",
            "Context (X): ['phenomena', 'is', 'non-random', ','] -> Target (Y): corpus\n",
            "Context (X): ['is', 'demonstrably', ',', 'does'] -> Target (Y): corpus\n",
            "Context (X): ['demonstrably', 'non-random', 'does', 'not'] -> Target (Y): corpus\n",
            "Context (X): ['non-random', ',', 'not', 'support'] -> Target (Y): corpus\n",
            "Context (X): [',', 'does', 'support', 'the'] -> Target (Y): corpus\n",
            "Context (X): ['does', 'not', 'the', 'inference'] -> Target (Y): corpus\n",
            "Context (X): ['not', 'support', 'inference', 'that'] -> Target (Y): corpus\n",
            "Context (X): ['support', 'the', 'that', 'it'] -> Target (Y): corpus\n",
            "Context (X): ['the', 'inference', 'it', 'is'] -> Target (Y): corpus\n",
            "Context (X): ['inference', 'that', 'is', 'not'] -> Target (Y): corpus\n",
            "Context (X): ['that', 'it', 'not', 'arbitrary'] -> Target (Y): corpus\n",
            "Context (X): ['it', 'is', 'arbitrary', '.'] -> Target (Y): not\n",
            "Context (X): ['is', 'not', '.', 'PAD'] -> Target (Y): arbitrary\n",
            "Context (X): ['not', 'arbitrary', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', 'present', 'experimental'] -> Target (Y): we\n",
            "Context (X): ['PAD', 'we', 'experimental', 'evidence'] -> Target (Y): present\n",
            "Context (X): ['we', 'present', 'evidence', 'of'] -> Target (Y): present\n",
            "Context (X): ['present', 'experimental', 'of', 'how'] -> Target (Y): present\n",
            "Context (X): ['experimental', 'evidence', 'how', 'arbitrary'] -> Target (Y): present\n",
            "Context (X): ['evidence', 'of', 'arbitrary', 'associations'] -> Target (Y): present\n",
            "Context (X): ['of', 'how', 'associations', 'between'] -> Target (Y): present\n",
            "Context (X): ['how', 'arbitrary', 'between', 'word'] -> Target (Y): present\n",
            "Context (X): ['arbitrary', 'associations', 'word', 'frequencies'] -> Target (Y): present\n",
            "Context (X): ['associations', 'between', 'frequencies', 'and'] -> Target (Y): present\n",
            "Context (X): ['between', 'word', 'and', 'corpora'] -> Target (Y): present\n",
            "Context (X): ['word', 'frequencies', 'corpora', 'are'] -> Target (Y): present\n",
            "Context (X): ['frequencies', 'and', 'are', 'systematically'] -> Target (Y): present\n",
            "Context (X): ['and', 'corpora', 'systematically', 'non-random'] -> Target (Y): present\n",
            "Context (X): ['corpora', 'are', 'non-random', '.'] -> Target (Y): systematically\n",
            "Context (X): ['are', 'systematically', '.', 'PAD'] -> Target (Y): non-random\n",
            "Context (X): ['systematically', 'non-random', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', 'review', 'literature'] -> Target (Y): we\n",
            "Context (X): ['PAD', 'we', 'literature', 'in'] -> Target (Y): review\n",
            "Context (X): ['we', 'review', 'in', 'which'] -> Target (Y): review\n",
            "Context (X): ['review', 'literature', 'which', 'hypothesis'] -> Target (Y): review\n",
            "Context (X): ['literature', 'in', 'hypothesis', 'testing'] -> Target (Y): review\n",
            "Context (X): ['in', 'which', 'testing', 'has'] -> Target (Y): review\n",
            "Context (X): ['which', 'hypothesis', 'has', 'been'] -> Target (Y): review\n",
            "Context (X): ['hypothesis', 'testing', 'been', 'used'] -> Target (Y): review\n",
            "Context (X): ['testing', 'has', 'used', ','] -> Target (Y): review\n",
            "Context (X): ['has', 'been', ',', 'and'] -> Target (Y): review\n",
            "Context (X): ['been', 'used', 'and', 'show'] -> Target (Y): review\n",
            "Context (X): ['used', ',', 'show', 'how'] -> Target (Y): review\n",
            "Context (X): [',', 'and', 'how', 'it'] -> Target (Y): review\n",
            "Context (X): ['and', 'show', 'it', 'has'] -> Target (Y): review\n",
            "Context (X): ['show', 'how', 'has', 'often'] -> Target (Y): review\n",
            "Context (X): ['how', 'it', 'often', 'led'] -> Target (Y): review\n",
            "Context (X): ['it', 'has', 'led', 'to'] -> Target (Y): review\n",
            "Context (X): ['has', 'often', 'to', 'unhelpful'] -> Target (Y): review\n",
            "Context (X): ['often', 'led', 'unhelpful', 'or'] -> Target (Y): review\n",
            "Context (X): ['led', 'to', 'or', 'misleading'] -> Target (Y): review\n",
            "Context (X): ['to', 'unhelpful', 'misleading', 'results'] -> Target (Y): review\n",
            "Context (X): ['unhelpful', 'or', 'results', '.'] -> Target (Y): misleading\n",
            "Context (X): ['or', 'misleading', '.', 'PAD'] -> Target (Y): results\n",
            "Context (X): ['misleading', 'results', 'PAD', 'PAD'] -> Target (Y): .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOABaoYDOKYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d64db6dc-b6f8-4bbc-943d-705f251a5629"
      },
      "source": [
        "# len(X)\n",
        "len(Y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH_iy4Lh1JUa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "31f99c15-1124-46fc-c69f-40b1810a1170"
      },
      "source": [
        "#define the model\n",
        "cbow = Sequential()\n",
        "###hint:output_dim = the shape of embedding matrix\n",
        "###hint:input_length = the length of training sample\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=144))\n",
        "cbow.add(Lambda(lambda x: relu(K.mean(x, axis=1)), output_shape=(embed_size,)))\n",
        "###hint:the total numbser of possible labels/words\n",
        "###hint:activation='softmax' or 'sigmoid'\n",
        "cbow.add(Dense(88, activation='sigmoid'))\n",
        "###hint:loss='categorical_crossentropy' or 'binary_crossentropy'\n",
        "cbow.compile(loss='binary_crossentropy', optimizer='sgd')\n",
        "cbow.summary()\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 144, 100)          8800      \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 88)                8888      \n",
            "=================================================================\n",
            "Total params: 17,688\n",
            "Trainable params: 17,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuVqi7WGDxNe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1730
        },
        "outputId": "fe178b17-2691-4c63-9b61-32261e48a358"
      },
      "source": [
        "#Train the model\n",
        "\n",
        "for epoch in range(100):\n",
        "    loss = 0.\n",
        "    for x, y in zip(X,Y):\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "    print(epoch, loss)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 144) for input Tensor(\"embedding_input:0\", shape=(None, 144), dtype=float32), but it was called on an input with incompatible shape (1, 4).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 144) for input Tensor(\"embedding_input:0\", shape=(None, 144), dtype=float32), but it was called on an input with incompatible shape (1, 4).\n",
            "0 99.48667031526566\n",
            "1 98.9187878370285\n",
            "2 98.35555642843246\n",
            "3 97.79694092273712\n",
            "4 97.24290537834167\n",
            "5 96.69340866804123\n",
            "6 96.14842212200165\n",
            "7 95.6078953742981\n",
            "8 95.0717870593071\n",
            "9 94.54005664587021\n",
            "10 94.01267874240875\n",
            "11 93.48961955308914\n",
            "12 92.97083234786987\n",
            "13 92.45628726482391\n",
            "14 91.94595175981522\n",
            "15 91.43976056575775\n",
            "16 90.93768805265427\n",
            "17 90.43970263004303\n",
            "18 89.94575506448746\n",
            "19 89.4558168053627\n",
            "20 88.96984475851059\n",
            "21 88.4877986907959\n",
            "22 88.00964426994324\n",
            "23 87.53535282611847\n",
            "24 87.06488889455795\n",
            "25 86.59820902347565\n",
            "26 86.13528454303741\n",
            "27 85.6760784983635\n",
            "28 85.2205656170845\n",
            "29 84.76870483160019\n",
            "30 84.32046282291412\n",
            "31 83.87580567598343\n",
            "32 83.43471425771713\n",
            "33 82.99714267253876\n",
            "34 82.5630350112915\n",
            "35 82.13236206769943\n",
            "36 81.70511472225189\n",
            "37 81.281259059906\n",
            "38 80.86076408624649\n",
            "39 80.44361734390259\n",
            "40 80.02979415655136\n",
            "41 79.61925864219666\n",
            "42 79.21195882558823\n",
            "43 78.80785584449768\n",
            "44 78.40693473815918\n",
            "45 78.00914978981018\n",
            "46 77.61446928977966\n",
            "47 77.222869515419\n",
            "48 76.83433371782303\n",
            "49 76.44884407520294\n",
            "50 76.06636029481888\n",
            "51 75.686832010746\n",
            "52 75.31025058031082\n",
            "53 74.93657940626144\n",
            "54 74.56579655408859\n",
            "55 74.1978867650032\n",
            "56 73.83283686637878\n",
            "57 73.47061258554459\n",
            "58 73.11117488145828\n",
            "59 72.75450485944748\n",
            "60 72.40058213472366\n",
            "61 72.04936888813972\n",
            "62 71.70084428787231\n",
            "63 71.35498982667923\n",
            "64 71.01177376508713\n",
            "65 70.67116537690163\n",
            "66 70.33314538002014\n",
            "67 69.99768766760826\n",
            "68 69.66476744413376\n",
            "69 69.33436584472656\n",
            "70 69.00645485520363\n",
            "71 68.6810173690319\n",
            "72 68.35802525281906\n",
            "73 68.03746545314789\n",
            "74 67.71930459141731\n",
            "75 67.40351989865303\n",
            "76 67.09009394049644\n",
            "77 66.7790094614029\n",
            "78 66.47023993730545\n",
            "79 66.16376292705536\n",
            "80 65.85956537723541\n",
            "81 65.55762976408005\n",
            "82 65.25793474912643\n",
            "83 64.96045622229576\n",
            "84 64.66517281532288\n",
            "85 64.37206056714058\n",
            "86 64.08108904957771\n",
            "87 63.79224565625191\n",
            "88 63.50550448894501\n",
            "89 63.22083759307861\n",
            "90 62.93823599815369\n",
            "91 62.65768280625343\n",
            "92 62.379152089357376\n",
            "93 62.10263204574585\n",
            "94 61.828105211257935\n",
            "95 61.55554926395416\n",
            "96 61.28494507074356\n",
            "97 61.016268879175186\n",
            "98 60.749506652355194\n",
            "99 60.484639286994934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxPVT_G-1RYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save the wordvectors\n",
        "f = open('Cbow_vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
        "vectors = cbow.get_weights()[0]\n",
        "for key in vocab:\n",
        "    str_vec = ' '.join(map(str, list(vectors[key, :])))\n",
        "    f.write('{} {}\\n'.format(vocab[key], str_vec))\n",
        "f.close()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1H3zTpE1Uwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "f20171b4-6d3a-4601-9833-4f3b40eedf2c"
      },
      "source": [
        "## Load the vectors back and validate\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./Cbow_vectors.txt', binary=False)\n",
        "\n",
        "w2v.most_similar(positive=['that'])\n",
        "w2v.most_similar(negative=['that'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(')', 0.21922579407691956),\n",
              " ('systematically', 0.1769963800907135),\n",
              " ('will', 0.13912953436374664),\n",
              " ('often', 0.13879328966140747),\n",
              " ('are', 0.13745608925819397),\n",
              " ('randomly', 0.12282629311084747),\n",
              " ('which', 0.10673969238996506),\n",
              " ('enough', 0.10435578972101212),\n",
              " ('able', 0.10381244122982025),\n",
              " ('show', 0.09622056037187576)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4z88oUR1Yk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "d3e71c9a-9cb2-4777-ae3b-4a460c47c18a"
      },
      "source": [
        "#Create Skipgram Training data \n",
        "\n",
        "# generate skip-grams with both positive and negative examples\n",
        "skip_grams = [skipgrams(sent, vocabulary_size=vocab_size, window_size=2) for sent in corpusByWordID]\n",
        "\n",
        "# view sample skip-grams\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(10):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "        vocab[pairs[i][0]], pairs[i][0],           \n",
        "        vocab[pairs[i][1]], pairs[i][1], \n",
        "        labels[i]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(language (6), experimental (68)) -> 0\n",
            "(. (1), results (82)) -> 0\n",
            "(essentially (4), testing (18)) -> 0\n",
            "(is (5), essentially (4)) -> 1\n",
            "(non-random (8), is (5)) -> 1\n",
            "(randomly (9), choose (3)) -> 1\n",
            "(language (6), or (81)) -> 0\n",
            "(essentially (4), enough (40)) -> 0\n",
            "(non-random (8), . (1)) -> 1\n",
            "(non-random (8), essentially (4)) -> 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqD40Iq11fpg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "4ef68e68-dc8e-4005-b98f-513dfd4d329f"
      },
      "source": [
        "#define the skip-gram model\n",
        "\n",
        "#define the model\n",
        "input_word = Input((1,))\n",
        "input_context_word = Input((1,))\n",
        "\n",
        "word_embedding = Embedding(input_dim=vocab_size, output_dim=100,input_length=1,name='word_embedding')\n",
        "context_embedding = Embedding(input_dim=vocab_size, output_dim=100,input_length=1,name='context_embedding')\n",
        "\n",
        "word_embedding = word_embedding(input_word)\n",
        "word_embedding_layer = Reshape((embed_size, 1))(word_embedding)\n",
        "\n",
        "context_embedding = context_embedding(input_context_word)\n",
        "context_embedding_layer = Reshape((embed_size, 1))(context_embedding)\n",
        "\n",
        "# now perform the dot product operation word_embedding_vec * context_embedding_vec\n",
        "dot_product = dot([word_embedding_layer, context_embedding_layer], axes=1)\n",
        "dot_product = Reshape((1,))(dot_product)\n",
        "\n",
        "###hint:the total number of possible labels/words\n",
        "###hint:activation='softmax' or 'sigmoid'\n",
        "outputLayer = Dense(1, activation='sigmoid')(dot_product)\n",
        "\n",
        "model = Model(inputs=[input_word, input_context_word], outputs=outputLayer)\n",
        "\n",
        "###hint:loss='categorical_crossentropy' or 'binary_crossentropy'\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# view model summary\n",
        "print(model.summary())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_embedding (Embedding)      (None, 1, 100)       8800        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conotext_embedding (Embedding)  (None, 1, 100)       8800        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 100, 1)       0           word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 100, 1)       0           conotext_embedding[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 1, 1)         0           reshape[0][0]                    \n",
            "                                                                 reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 1)            0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            2           reshape_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 17,602\n",
            "Trainable params: 17,602\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVpBqKfo1iSj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3343
        },
        "outputId": "1c87c65e-ae54-4160-d864-9af59c5a2627"
      },
      "source": [
        "#train the model\n",
        "\n",
        "for epoch in range(1, 100):\n",
        "    loss = 0\n",
        "    for i, elem in enumerate(skip_grams):\n",
        "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "        labels = np.array(elem[1], dtype='int32')\n",
        "        X = [pair_first_elem, pair_second_elem]\n",
        "        Y = labels\n",
        "        if i % 10000 == 0:\n",
        "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
        "        loss += model.train_on_batch(X,Y)  \n",
        "\n",
        "    print('Epoch:', epoch, 'Loss:', loss)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 1 Loss: 4.852185547351837\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 2 Loss: 4.8475106954574585\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 3 Loss: 4.843109667301178\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 4 Loss: 4.838240742683411\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 5 Loss: 4.832616865634918\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 6 Loss: 4.825949430465698\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 7 Loss: 4.817927241325378\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 8 Loss: 4.808212757110596\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 9 Loss: 4.796443045139313\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 10 Loss: 4.782229721546173\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 11 Loss: 4.765164613723755\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 12 Loss: 4.744821846485138\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 13 Loss: 4.720767617225647\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 14 Loss: 4.692565977573395\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 15 Loss: 4.659789681434631\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 16 Loss: 4.622031033039093\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 17 Loss: 4.5789124965667725\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 18 Loss: 4.530099451541901\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 19 Loss: 4.47531133890152\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 20 Loss: 4.414334416389465\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 21 Loss: 4.3470324873924255\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 22 Loss: 4.273357689380646\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 23 Loss: 4.193357586860657\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 24 Loss: 4.107183039188385\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 25 Loss: 4.015090703964233\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 26 Loss: 3.917443335056305\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 27 Loss: 3.8147066235542297\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 28 Loss: 3.707441419363022\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 29 Loss: 3.5962919294834137\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 30 Loss: 3.481971025466919\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 31 Loss: 3.3652412593364716\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 32 Loss: 3.2468952238559723\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 33 Loss: 3.1277339458465576\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 34 Loss: 3.0085447430610657\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 35 Loss: 2.890081912279129\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 36 Loss: 2.7730490267276764\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 37 Loss: 2.658084064722061\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 38 Loss: 2.5457496643066406\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 39 Loss: 2.4365262389183044\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 40 Loss: 2.3308087289333344\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 41 Loss: 2.228908449411392\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 42 Loss: 2.131055533885956\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 43 Loss: 2.0374066531658173\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 44 Loss: 1.948050707578659\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 45 Loss: 1.8630190640687943\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 46 Loss: 1.7822923362255096\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 47 Loss: 1.7058106660842896\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 48 Loss: 1.633480191230774\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 49 Loss: 1.5651815086603165\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 50 Loss: 1.5007752925157547\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 51 Loss: 1.4401078522205353\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 52 Loss: 1.3830162435770035\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 53 Loss: 1.3293315321207047\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 54 Loss: 1.2788830697536469\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 55 Loss: 1.2314997613430023\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 56 Loss: 1.1870129704475403\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 57 Loss: 1.1452576518058777\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 58 Loss: 1.106073759496212\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 59 Loss: 1.069306991994381\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 60 Loss: 1.034809410572052\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 61 Loss: 1.0024397745728493\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 62 Loss: 0.9720638766884804\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 63 Loss: 0.9435544535517693\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 64 Loss: 0.916791282594204\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 65 Loss: 0.8916607946157455\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 66 Loss: 0.8680564388632774\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 67 Loss: 0.8458779007196426\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 68 Loss: 0.8250314220786095\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 69 Loss: 0.8054288849234581\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 70 Loss: 0.7869880348443985\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 71 Loss: 0.769631914794445\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 72 Loss: 0.7532888948917389\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 73 Loss: 0.7378919199109077\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 74 Loss: 0.723378486931324\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 75 Loss: 0.7096905335783958\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 76 Loss: 0.6967738717794418\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 77 Loss: 0.6845779791474342\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 78 Loss: 0.6730559393763542\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 79 Loss: 0.6621641255915165\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 80 Loss: 0.6518617756664753\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 81 Loss: 0.6421110965311527\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 82 Loss: 0.6328769885003567\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 83 Loss: 0.624126672744751\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 84 Loss: 0.6158297024667263\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 85 Loss: 0.6079577654600143\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 86 Loss: 0.6004844531416893\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 87 Loss: 0.5933851972222328\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 88 Loss: 0.5866372138261795\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 89 Loss: 0.5802192538976669\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 90 Loss: 0.5741113759577274\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 91 Loss: 0.5682952664792538\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 92 Loss: 0.562753651291132\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 93 Loss: 0.5574704259634018\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 94 Loss: 0.55243069678545\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 95 Loss: 0.5476204454898834\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 96 Loss: 0.5430266223847866\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 97 Loss: 0.5386370830237865\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 98 Loss: 0.5344403684139252\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 99 Loss: 0.530425850301981\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjhkbLsp1k0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the embeding matrix\n",
        "weights = model.get_weights()\n",
        "## Save the wordvectors\n",
        "f = open('skipgram_vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
        "vectors = model.get_weights()[0]\n",
        "for key in vocab:\n",
        "    str_vec = ' '.join(map(str, list(vectors[key, :])))\n",
        "    f.write('{} {}\\n'.format(vocab[key], str_vec))\n",
        "f.close()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4rUPCJC1mvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "80dc44c3-c68f-468b-9425-0a9573fc4b51"
      },
      "source": [
        "## Load the vectors back and validate\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./skipgram_vectors.txt', binary=False)\n",
        "w2v.most_similar(positive=['the'])\n",
        "w2v.most_similar(negative=['the'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('almost', 0.30979210138320923),\n",
              " ('misleading', 0.2943827509880066),\n",
              " ('experimental', 0.274384081363678),\n",
              " ('of', 0.2254631221294403),\n",
              " ('unhelpful', 0.22091346979141235),\n",
              " ('evidence', 0.20839455723762512),\n",
              " ('corpora', 0.20792268216609955),\n",
              " ('studies', 0.19628621637821198),\n",
              " ('relation', 0.18958404660224915),\n",
              " ('or', 0.18859079480171204)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs8ng_Zf1o08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Excerise: modeify the skipegram_model to share the same embeding layer between word and context\n",
        "#Discussion: which is better? Why?  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}