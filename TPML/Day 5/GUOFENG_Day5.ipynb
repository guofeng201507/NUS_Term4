{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GUOFENG_Day5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm9LGNojT33P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "0edcde68-289a-4bbc-b53f-2fd90b3ea65f"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 18.2MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 19.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 31.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=11d78e8d5c4383a29059852962a7b84729b650528d7a87a0b9cddabc57308488\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj_w9N1YX4aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWtYk7x_UZRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmXobdNEVRCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPAtXMNFWInd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL1PC9wPXAAh",
        "colab_type": "text"
      },
      "source": [
        "### 3 sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYGfMy3kW-H7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text1= \"What’s the time now in Singapore?\"\n",
        "text2= \"What is the weather in Seattle today?\"\n",
        "text3= \"Apple is looking at buying the U.K. startup for $1 billion.\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga9LXN1oW7YH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sent_embedding(text):\n",
        "    # Add the special tokens.\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "    # Split the sentence into tokens.\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "    # Map the token strings to their vocabulary indeces.\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, segments_tensors)\n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "    token_vecs = hidden_states[-4][0]\n",
        "\n",
        "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "\n",
        "    return sentence_embedding\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV1FAmJmcWJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_embedding_text1 = get_sent_embedding(text1)\n",
        "sentence_embedding_text2 = get_sent_embedding(text2)\n",
        "sentence_embedding_text3 = get_sent_embedding(text3)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmaFbEGBYUYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cosine\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8yHEMv66eZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simi_1vs2 = 1 - cosine(sentence_embedding_text1, sentence_embedding_text2)\n",
        "simi_1vs3 = 1 - cosine(sentence_embedding_text1, sentence_embedding_text3)\n",
        "simi_2vs3 = 1 - cosine(sentence_embedding_text2, sentence_embedding_text3)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAf2GyuBBhUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "e5c8ef04-0b41-4a53-b6f2-cd1c8fa612d2"
      },
      "source": [
        "print(f\"Similarity between sentence 1 to sentence 2 is  {simi_1vs2}\")\n",
        "print(f\"Similarity between sentence 1 to sentence 3 is  {simi_1vs3}\")\n",
        "print(f\"Similarity between sentence 2 to sentence 3 is  {simi_2vs3}\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity between sentence 1 to sentence 2 is  0.8359468579292297\n",
            "Similarity between sentence 1 to sentence 3 is  0.5442742109298706\n",
            "Similarity between sentence 2 to sentence 3 is  0.5663244724273682\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}