{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Doc2Vec Model\n",
    "=============\n",
    "\n",
    "Introduces Gensim's Doc2Vec model and demonstrates its use on the Lee Corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is a `core_concepts_model` that represents each\n",
    "`core_concepts_document` as a `core_concepts_vector`.  This\n",
    "tutorial introduces the model and demonstrates how to train and assess it.\n",
    "\n",
    "Here's a list of what we'll be doing:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "Review: Bag-of-words\n",
    "--------------------\n",
    "\n",
    ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
    "\n",
    "You may be familiar with the `bag-of-words model\n",
    "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n",
    "`core_concepts_vector` section.\n",
    "This model transforms each document to a fixed-length vector of integers.\n",
    "For example, given the sentences:\n",
    "\n",
    "- ``John likes to watch movies. Mary likes movies too.``\n",
    "- ``John also likes to watch football games. Mary hates football.``\n",
    "\n",
    "The model outputs the vectors:\n",
    "\n",
    "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
    "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
    "\n",
    "Each vector has 10 elements, where each element counts the number of times a\n",
    "particular word occurred in the document.\n",
    "The order of elements is arbitrary.\n",
    "In the example above, the order of the elements corresponds to the words:\n",
    "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
    "\n",
    "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
    "\n",
    "First, they lose all information about word order: \"John likes Mary\" and\n",
    "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
    "of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\n",
    "models consider word phrases of length n to represent documents as\n",
    "fixed-length vectors to capture local word order but suffer from data\n",
    "sparsity and high dimensionality.\n",
    "\n",
    "Second, the model does not attempt to learn the meaning of the underlying\n",
    "words, and as a consequence, the distance between vectors doesn't always\n",
    "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
    "second problem.\n",
    "\n",
    "Review: ``Word2Vec`` Model\n",
    "--------------------------\n",
    "\n",
    "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
    "vector space using a shallow neural network. The result is a set of\n",
    "word-vectors where vectors close together in vector space have similar\n",
    "meanings based on context, and word-vectors distant to each other have\n",
    "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
    "together and ``strong`` and ``Paris`` would be relatively far.\n",
    "\n",
    "Gensim's :py:class:`~gensim.models.word2vec.Word2Vec` class implements this model.\n",
    "\n",
    "With the ``Word2Vec`` model, we can calculate the vectors for each **word** in a document.\n",
    "But what if we want to calculate a vector for the **entire document**\\ ?\n",
    "We could average the vectors for each word in the document - while this is quick and crude, it can often be useful.\n",
    "However, there is a better way...\n",
    "\n",
    "Introducing: Paragraph Vector\n",
    "-----------------------------\n",
    "\n",
    ".. Important:: In Gensim, we refer to the Paragraph Vector model as ``Doc2Vec``.\n",
    "\n",
    "Le and Mikolov in 2014 introduced the `Doc2Vec algorithm <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__, which usually outperforms such simple-averaging of ``Word2Vec`` vectors.\n",
    "\n",
    "The basic idea is: act as if a document has another floating word-like\n",
    "vector, which contributes to all training predictions, and is updated like\n",
    "other word-vectors, but we will call it a doc-vector. Gensim's\n",
    ":py:class:`~gensim.models.doc2vec.Doc2Vec` class implements this algorithm.\n",
    "\n",
    "There are two implementations:\n",
    "\n",
    "1. Paragraph Vector - Distributed Memory (PV-DM)\n",
    "2. Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "\n",
    ".. Important::\n",
    "  Don't let the implementation details below scare you.\n",
    "  They're advanced material: if it's too much, then move on to the next section.\n",
    "\n",
    "PV-DM is analogous to Word2Vec CBOW. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a center word based an\n",
    "average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "PV-DBOW is analogous to Word2Vec SG. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a target word just from\n",
    "the full document's doc-vector. (It is also common to combine this with\n",
    "skip-gram testing, using both the doc-vector and nearby word-vectors to\n",
    "predict a single target word, but only one at a time.)\n",
    "\n",
    "Prepare the Training and Test Data\n",
    "----------------------------------\n",
    "\n",
    "For this tutorial, we'll be training our model using the `Lee Background\n",
    "Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "included in gensim. This corpus contains 314 documents selected from the\n",
    "Australian Broadcasting Corporation’s news mail service, which provides text\n",
    "e-mails of headline stories and covers a number of broad topics.\n",
    "\n",
    "And we'll test our model by eye using the much shorter `Lee Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "which contains 50 documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Function to Read and Preprocess Text\n",
    "---------------------------------------------\n",
    "\n",
    "Below, we define a function to:\n",
    "\n",
    "- open the train/test file (with latin encoding)\n",
    "- read the file line-by-line\n",
    "- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "\n",
    "The file we're reading is a **corpus**.\n",
    "Each line of the file is a **document**.\n",
    "\n",
    ".. Important::\n",
    "  To train the model, we'll need to associate a tag/number with each document\n",
    "  of the training corpus. In our case, the tag is simply the zero-based line\n",
    "  number.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern', 'highlands', 'of', 'new', 'south', 'wales', 'as', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'the', 'town', 'of', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'of', 'sydney', 'has', 'forced', 'the', 'closure', 'of', 'the', 'hume', 'highway', 'at', 'about', 'pm', 'aedt', 'marked', 'deterioration', 'in', 'the', 'weather', 'as', 'storm', 'cell', 'moved', 'east', 'across', 'the', 'blue', 'mountains', 'forced', 'authorities', 'to', 'make', 'decision', 'to', 'evacuate', 'people', 'from', 'homes', 'in', 'outlying', 'streets', 'at', 'hill', 'top', 'in', 'the', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', 'residents', 'have', 'left', 'their', 'homes', 'for', 'nearby', 'mittagong', 'the', 'new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'the', 'weather', 'conditions', 'which', 'caused', 'the', 'fire', 'to', 'burn', 'in', 'finger', 'formation', 'have', 'now', 'eased', 'and', 'about', 'fire', 'units', 'in', 'and', 'around', 'hill', 'top', 'are', 'optimistic', 'of', 'defending', 'all', 'properties', 'as', 'more', 'than', 'blazes', 'burn', 'on', 'new', 'year', 'eve', 'in', 'new', 'south', 'wales', 'fire', 'crews', 'have', 'been', 'called', 'to', 'new', 'fire', 'at', 'gunning', 'south', 'of', 'goulburn', 'while', 'few', 'details', 'are', 'available', 'at', 'this', 'stage', 'fire', 'authorities', 'says', 'it', 'has', 'closed', 'the', 'hume', 'highway', 'in', 'both', 'directions', 'meanwhile', 'new', 'fire', 'in', 'sydney', 'west', 'is', 'no', 'longer', 'threatening', 'properties', 'in', 'the', 'cranebrook', 'area', 'rain', 'has', 'fallen', 'in', 'some', 'parts', 'of', 'the', 'illawarra', 'sydney', 'the', 'hunter', 'valley', 'and', 'the', 'north', 'coast', 'but', 'the', 'bureau', 'of', 'meteorology', 'claire', 'richards', 'says', 'the', 'rain', 'has', 'done', 'little', 'to', 'ease', 'any', 'of', 'the', 'hundred', 'fires', 'still', 'burning', 'across', 'the', 'state', 'the', 'falls', 'have', 'been', 'quite', 'isolated', 'in', 'those', 'areas', 'and', 'generally', 'the', 'falls', 'have', 'been', 'less', 'than', 'about', 'five', 'millimetres', 'she', 'said', 'in', 'some', 'places', 'really', 'not', 'significant', 'at', 'all', 'less', 'than', 'millimetre', 'so', 'there', 'hasn', 'been', 'much', 'relief', 'as', 'far', 'as', 'rain', 'is', 'concerned', 'in', 'fact', 'they', 've', 'probably', 'hampered', 'the', 'efforts', 'of', 'the', 'firefighters', 'more', 'because', 'of', 'the', 'wind', 'gusts', 'that', 'are', 'associated', 'with', 'those', 'thunderstorms'], tags=[0]), TaggedDocument(words=['indian', 'security', 'forces', 'have', 'shot', 'dead', 'eight', 'suspected', 'militants', 'in', 'night', 'long', 'encounter', 'in', 'southern', 'kashmir', 'the', 'shootout', 'took', 'place', 'at', 'dora', 'village', 'some', 'kilometers', 'south', 'of', 'the', 'kashmiri', 'summer', 'capital', 'srinagar', 'the', 'deaths', 'came', 'as', 'pakistani', 'police', 'arrested', 'more', 'than', 'two', 'dozen', 'militants', 'from', 'extremist', 'groups', 'accused', 'of', 'staging', 'an', 'attack', 'on', 'india', 'parliament', 'india', 'has', 'accused', 'pakistan', 'based', 'lashkar', 'taiba', 'and', 'jaish', 'mohammad', 'of', 'carrying', 'out', 'the', 'attack', 'on', 'december', 'at', 'the', 'behest', 'of', 'pakistani', 'military', 'intelligence', 'military', 'tensions', 'have', 'soared', 'since', 'the', 'raid', 'with', 'both', 'sides', 'massing', 'troops', 'along', 'their', 'border', 'and', 'trading', 'tit', 'for', 'tat', 'diplomatic', 'sanctions', 'yesterday', 'pakistan', 'announced', 'it', 'had', 'arrested', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'in', 'karachi', 'say', 'it', 'is', 'likely', 'more', 'raids', 'will', 'be', 'launched', 'against', 'the', 'two', 'groups', 'as', 'well', 'as', 'other', 'militant', 'organisations', 'accused', 'of', 'targetting', 'india', 'military', 'tensions', 'between', 'india', 'and', 'pakistan', 'have', 'escalated', 'to', 'level', 'not', 'seen', 'since', 'their', 'war'], tags=[1])]\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'national', 'executive', 'of', 'the', 'strife', 'torn', 'democrats', 'last', 'night', 'appointed', 'little', 'known', 'west', 'australian', 'senator', 'brian', 'greig', 'as', 'interim', 'leader', 'shock', 'move', 'likely', 'to', 'provoke', 'further', 'conflict', 'between', 'the', 'party', 'senators', 'and', 'its', 'organisation', 'in', 'move', 'to', 'reassert', 'control', 'over', 'the', 'party', 'seven', 'senators', 'the', 'national', 'executive', 'last', 'night', 'rejected', 'aden', 'ridgeway', 'bid', 'to', 'become', 'interim', 'leader', 'in', 'favour', 'of', 'senator', 'greig', 'supporter', 'of', 'deposed', 'leader', 'natasha', 'stott', 'despoja', 'and', 'an', 'outspoken', 'gay', 'rights', 'activist'], ['cash', 'strapped', 'financial', 'services', 'group', 'amp', 'has', 'shelved', 'million', 'plan', 'to', 'buy', 'shares', 'back', 'from', 'investors', 'and', 'will', 'raise', 'million', 'in', 'fresh', 'capital', 'after', 'profits', 'crashed', 'in', 'the', 'six', 'months', 'to', 'june', 'chief', 'executive', 'paul', 'batchelor', 'said', 'the', 'result', 'was', 'solid', 'in', 'what', 'he', 'described', 'as', 'the', 'worst', 'conditions', 'for', 'stock', 'markets', 'in', 'years', 'amp', 'half', 'year', 'profit', 'sank', 'per', 'cent', 'to', 'million', 'or', 'share', 'as', 'australia', 'largest', 'investor', 'and', 'fund', 'manager', 'failed', 'to', 'hit', 'projected', 'per', 'cent', 'earnings', 'growth', 'targets', 'and', 'was', 'battered', 'by', 'falling', 'returns', 'on', 'share', 'markets']]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain\n",
    "any tags.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "Now, we'll instantiate a Doc2Vec model with a vector size with 50 dimensions and\n",
    "iterating over the training corpus 40 times. We set the minimum word count to\n",
    "2 in order to discard words with very few occurrences. (Without a variety of\n",
    "representative examples, retaining such infrequent words can often make a\n",
    "model worse!) Typical iteration counts in the published `Paragraph Vector paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__\n",
    "results, using 10s-of-thousands to millions of docs, are 10-20. More\n",
    "iterations take more time and eventually reach a point of diminishing\n",
    "returns.\n",
    "\n",
    "However, this is a very very small dataset (300 documents) with shortish\n",
    "documents (a few hundred words). Adding training passes can sometimes help\n",
    "with such small datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-19 15:34:52,182 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-19 15:34:57,144 : INFO : collecting all words and their counts\n",
      "2020-08-19 15:34:57,145 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-08-19 15:34:57,153 : INFO : collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "2020-08-19 15:34:57,154 : INFO : Loading a fresh vocabulary\n",
      "2020-08-19 15:34:57,162 : INFO : effective_min_count=2 retains 3955 unique words (56% of original 6981, drops 3026)\n",
      "2020-08-19 15:34:57,163 : INFO : effective_min_count=2 leaves 55126 word corpus (94% of original 58152, drops 3026)\n",
      "2020-08-19 15:34:57,174 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2020-08-19 15:34:57,175 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2020-08-19 15:34:57,176 : INFO : downsampling leaves estimated 42390 word corpus (76.9% of prior 55126)\n",
      "2020-08-19 15:34:57,184 : INFO : estimated required memory for 3955 words and 50 dimensions: 3619500 bytes\n",
      "2020-08-19 15:34:57,185 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a dictionary (accessible via\n",
    "``model.wv.vocab``\\ ) of all of the unique words extracted from the training\n",
    "corpus along with the count (e.g., ``model.wv.vocab['penalty'].count`` for\n",
    "counts for the word ``penalty``\\ ).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the model on the corpus.\n",
    "If the BLAS library is being used, this should take no more than 3 seconds.\n",
    "If the BLAS library is not being used, this should take no more than 2\n",
    "minutes, so use BLAS if you value your time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-19 15:35:20,437 : INFO : training model with 3 workers on 3955 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-08-19 15:35:20,482 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:20,484 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:20,485 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:20,486 : INFO : EPOCH - 1 : training on 58152 raw words (42626 effective words) took 0.0s, 907459 effective words/s\n",
      "2020-08-19 15:35:20,530 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:20,532 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:20,534 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:20,534 : INFO : EPOCH - 2 : training on 58152 raw words (42717 effective words) took 0.0s, 929453 effective words/s\n",
      "2020-08-19 15:35:20,570 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:20,574 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:20,575 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:20,576 : INFO : EPOCH - 3 : training on 58152 raw words (42824 effective words) took 0.0s, 1093272 effective words/s\n",
      "2020-08-19 15:35:20,615 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:20,617 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:20,619 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:20,620 : INFO : EPOCH - 4 : training on 58152 raw words (42658 effective words) took 0.0s, 1040059 effective words/s\n",
      "2020-08-19 15:35:20,663 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:20,667 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:20,668 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:20,668 : INFO : EPOCH - 5 : training on 58152 raw words (42628 effective words) took 0.0s, 917718 effective words/s\n",
      "2020-08-19 15:35:20,710 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:20,713 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:20,716 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:20,716 : INFO : EPOCH - 6 : training on 58152 raw words (42661 effective words) took 0.0s, 933127 effective words/s\n",
      "2020-08-19 15:35:20,755 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:20,758 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:20,759 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:20,760 : INFO : EPOCH - 7 : training on 58152 raw words (42621 effective words) took 0.0s, 1035873 effective words/s\n",
      "2020-08-19 15:35:20,802 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:20,804 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:20,805 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:20,806 : INFO : EPOCH - 8 : training on 58152 raw words (42630 effective words) took 0.0s, 1002210 effective words/s\n",
      "2020-08-19 15:35:20,843 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:20,844 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:20,848 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:20,849 : INFO : EPOCH - 9 : training on 58152 raw words (42635 effective words) took 0.0s, 1049017 effective words/s\n",
      "2020-08-19 15:35:20,889 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:20,891 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:20,893 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:20,893 : INFO : EPOCH - 10 : training on 58152 raw words (42776 effective words) took 0.0s, 1015030 effective words/s\n",
      "2020-08-19 15:35:20,938 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:20,939 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:20,942 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:20,942 : INFO : EPOCH - 11 : training on 58152 raw words (42762 effective words) took 0.0s, 910913 effective words/s\n",
      "2020-08-19 15:35:20,982 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:20,985 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:20,986 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:20,986 : INFO : EPOCH - 12 : training on 58152 raw words (42610 effective words) took 0.0s, 1023639 effective words/s\n",
      "2020-08-19 15:35:21,030 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,032 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,034 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,034 : INFO : EPOCH - 13 : training on 58152 raw words (42744 effective words) took 0.0s, 933424 effective words/s\n",
      "2020-08-19 15:35:21,076 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,077 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,078 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,078 : INFO : EPOCH - 14 : training on 58152 raw words (42699 effective words) took 0.0s, 1032332 effective words/s\n",
      "2020-08-19 15:35:21,118 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,121 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,122 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,122 : INFO : EPOCH - 15 : training on 58152 raw words (42636 effective words) took 0.0s, 1020076 effective words/s\n",
      "2020-08-19 15:35:21,158 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,160 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,161 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,162 : INFO : EPOCH - 16 : training on 58152 raw words (42713 effective words) took 0.0s, 1143634 effective words/s\n",
      "2020-08-19 15:35:21,202 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,204 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,205 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,206 : INFO : EPOCH - 17 : training on 58152 raw words (42598 effective words) took 0.0s, 1018046 effective words/s\n",
      "2020-08-19 15:35:21,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,245 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,247 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,247 : INFO : EPOCH - 18 : training on 58152 raw words (42616 effective words) took 0.0s, 1080571 effective words/s\n",
      "2020-08-19 15:35:21,285 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,289 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,290 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,290 : INFO : EPOCH - 19 : training on 58152 raw words (42707 effective words) took 0.0s, 1026455 effective words/s\n",
      "2020-08-19 15:35:21,331 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,332 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,333 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,334 : INFO : EPOCH - 20 : training on 58152 raw words (42583 effective words) took 0.0s, 1024164 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-19 15:35:21,373 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,375 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,377 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,377 : INFO : EPOCH - 21 : training on 58152 raw words (42703 effective words) took 0.0s, 1036144 effective words/s\n",
      "2020-08-19 15:35:21,418 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,419 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,420 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,420 : INFO : EPOCH - 22 : training on 58152 raw words (42825 effective words) took 0.0s, 1088625 effective words/s\n",
      "2020-08-19 15:35:21,462 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,463 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,464 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,465 : INFO : EPOCH - 23 : training on 58152 raw words (42663 effective words) took 0.0s, 1013717 effective words/s\n",
      "2020-08-19 15:35:21,505 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,508 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,509 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,509 : INFO : EPOCH - 24 : training on 58152 raw words (42641 effective words) took 0.0s, 1001597 effective words/s\n",
      "2020-08-19 15:35:21,546 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,548 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,549 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,550 : INFO : EPOCH - 25 : training on 58152 raw words (42664 effective words) took 0.0s, 1116905 effective words/s\n",
      "2020-08-19 15:35:21,588 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,590 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,592 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,593 : INFO : EPOCH - 26 : training on 58152 raw words (42693 effective words) took 0.0s, 1063722 effective words/s\n",
      "2020-08-19 15:35:21,632 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,635 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,636 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,637 : INFO : EPOCH - 27 : training on 58152 raw words (42685 effective words) took 0.0s, 1012505 effective words/s\n",
      "2020-08-19 15:35:21,674 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,675 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,677 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,678 : INFO : EPOCH - 28 : training on 58152 raw words (42809 effective words) took 0.0s, 1111351 effective words/s\n",
      "2020-08-19 15:35:21,718 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,720 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,721 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,721 : INFO : EPOCH - 29 : training on 58152 raw words (42741 effective words) took 0.0s, 1036877 effective words/s\n",
      "2020-08-19 15:35:21,757 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,758 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,760 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,760 : INFO : EPOCH - 30 : training on 58152 raw words (42706 effective words) took 0.0s, 1147924 effective words/s\n",
      "2020-08-19 15:35:21,799 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,800 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,801 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,801 : INFO : EPOCH - 31 : training on 58152 raw words (42705 effective words) took 0.0s, 1099856 effective words/s\n",
      "2020-08-19 15:35:21,839 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,840 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,841 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,842 : INFO : EPOCH - 32 : training on 58152 raw words (42722 effective words) took 0.0s, 1111206 effective words/s\n",
      "2020-08-19 15:35:21,879 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,881 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,882 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,882 : INFO : EPOCH - 33 : training on 58152 raw words (42649 effective words) took 0.0s, 1099363 effective words/s\n",
      "2020-08-19 15:35:21,918 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,920 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,922 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,923 : INFO : EPOCH - 34 : training on 58152 raw words (42704 effective words) took 0.0s, 1128228 effective words/s\n",
      "2020-08-19 15:35:21,962 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:21,963 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:21,965 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:21,966 : INFO : EPOCH - 35 : training on 58152 raw words (42683 effective words) took 0.0s, 1031484 effective words/s\n",
      "2020-08-19 15:35:22,004 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:22,007 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:22,008 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:22,008 : INFO : EPOCH - 36 : training on 58152 raw words (42707 effective words) took 0.0s, 1091408 effective words/s\n",
      "2020-08-19 15:35:22,045 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:22,048 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:22,049 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:22,050 : INFO : EPOCH - 37 : training on 58152 raw words (42625 effective words) took 0.0s, 1084462 effective words/s\n",
      "2020-08-19 15:35:22,089 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:22,091 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:22,091 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:22,092 : INFO : EPOCH - 38 : training on 58152 raw words (42657 effective words) took 0.0s, 1061357 effective words/s\n",
      "2020-08-19 15:35:22,135 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:22,137 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:22,138 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:22,138 : INFO : EPOCH - 39 : training on 58152 raw words (42753 effective words) took 0.0s, 966464 effective words/s\n",
      "2020-08-19 15:35:22,177 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-19 15:35:22,178 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-19 15:35:22,178 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-19 15:35:22,178 : INFO : EPOCH - 40 : training on 58152 raw words (42640 effective words) took 0.0s, 1115664 effective words/s\n",
      "2020-08-19 15:35:22,180 : INFO : training on a 2326080 raw words (1707419 effective words) took 1.7s, 981555 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the trained model to infer a vector for any piece of text\n",
    "by passing a list of words to the ``model.infer_vector`` function. This\n",
    "vector can then be compared with other vectors via cosine similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01490097  0.02111813 -0.03109653 -0.01068063 -0.01892539 -0.3459541\n",
      "  0.08585836  0.05513358  0.23179501 -0.03962927 -0.04393085 -0.26241705\n",
      "  0.1303937  -0.07906086 -0.19340687 -0.08851517 -0.12310801 -0.14102398\n",
      "  0.0950866  -0.03843781 -0.01226297  0.13106963  0.04221071  0.14866176\n",
      " -0.11223336 -0.06549908  0.28320494  0.11926895 -0.27281046 -0.12527478\n",
      "  0.0416162   0.25637335 -0.10573249  0.10783476  0.335377    0.0543611\n",
      " -0.20440938  0.14640705  0.01357154 -0.08848789  0.0082607   0.15403083\n",
      "  0.01400404  0.05685166  0.04151241  0.05091893  0.04578988 -0.04650984\n",
      " -0.09706578  0.30933914]\n"
     ]
    }
   ],
   "source": [
    "vector1 = model.infer_vector(['Soccer', 'is', 'a', 'popular', 'sport', 'always'])\n",
    "print(vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09620431  0.08249308 -0.0395282  -0.01915498  0.07021865 -0.18614933\n",
      "  0.16727541  0.07942826  0.18905391 -0.18667327 -0.03328803 -0.10956103\n",
      "  0.11144634 -0.15424423 -0.10160239 -0.0665291  -0.23383522  0.01524642\n",
      " -0.03415006 -0.0788225   0.00722739  0.10200074  0.15445967  0.03229436\n",
      " -0.14097126  0.00884841  0.27664086  0.0468643  -0.16139928 -0.0074278\n",
      " -0.04843001  0.25789395 -0.15133135  0.1747285   0.19457853  0.114756\n",
      " -0.2919086   0.02184635  0.0183157  -0.129457    0.08781081  0.01398888\n",
      " -0.07676763  0.03834012  0.03356643  0.0736498   0.1731747  -0.01246149\n",
      " -0.08871602  0.23394802]\n"
     ]
    }
   ],
   "source": [
    "vector2 = model.infer_vector(['Football', 'is', 'very', 'fun', 'to', 'play'])\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.468309050425887\n"
     ]
    }
   ],
   "source": [
    "res = vector1 - vector2\n",
    "import numpy as np\n",
    "import math\n",
    "res = sum(abs(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``infer_vector()`` does *not* take a string, but rather a list of\n",
    "string tokens, which should have already been tokenized the same way as the\n",
    "``words`` property of original training document objects.\n",
    "\n",
    "Also note that because the underlying training/inference algorithms are an\n",
    "iterative approximation problem that makes use of internal randomization,\n",
    "repeated inferences of the same text will return slightly different vectors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing the Model\n",
    "-------------------\n",
    "\n",
    "To assess our new model, we'll first infer new vectors for each document of\n",
    "the training corpus, compare the inferred vectors with the training corpus,\n",
    "and then returning the rank of the document based on self-similarity.\n",
    "Basically, we're pretending as if the training corpus is some new unseen data\n",
    "and then seeing how they compare with the trained model. The expectation is\n",
    "that we've likely overfit our model (i.e., all of the ranks will be less than\n",
    "2) and so we should be able to find similar documents very easily.\n",
    "Additionally, we'll keep track of the second ranks for a comparison of less\n",
    "similar documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-19 15:42:55,033 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how each document ranks with respect to the training corpus\n",
    "\n",
    "NB. Results vary between runs due to random seeding and very small corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 293, 1: 7})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, greater than 95% of the inferred documents are found to be most\n",
    "similar to itself and about 5% of the time it is mistakenly most similar to\n",
    "another document. Checking the inferred-vector against a\n",
    "training-vector is a sort of 'sanity check' as to whether the model is\n",
    "behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "\n",
    "This is great and not entirely surprising. We can take a look at an example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (299): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (299, 0.937798023223877): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
      "\n",
      "SECOND-MOST (146, 0.8162227272987366): «the australian and south african sides for the first cricket test starting at the adelaide oval today are not expected to be finalised until just before the start of play australian captain steve waugh and his south african counterpart shaun pollock will decide on their lineups after an inspection of the pitch shortly before the start of play the match holds special significance for waugh and his twin brother mark who play their th test together steve waugh is not placing too much relevance on the milestone don want to read too much into it guess and then get too carried away but later on when we retire and look back on it it will be significant it nice for the family mum and dad all the sacrifices they made you know with us growing up and also our brothers so you know it nice for the family he said»\n",
      "\n",
      "MEDIAN (24, 0.2568298876285553): «team of police is currently escorting two swiss tourists back to the safety of central australian community after their vehicle sank in sand in the finke river overnight police spokesman says police were called to the area kilometres west of alice springs after an emergency beacon was activated and received by australian search and rescue the tourists had tried to cross the river when their vehicle became submerged in soft sand the spokesman says ground unit also attended but police officers had to walk the final kilometres to reach the stranded swiss nationals who were stuck in the vehicle the tourists and the rescue team are expected to walk for about four hours this morning before driving to the hermansburg community»\n",
      "\n",
      "LEAST (1, -0.07719731330871582): «indian security forces have shot dead eight suspected militants in night long encounter in southern kashmir the shootout took place at dora village some kilometers south of the kashmiri summer capital srinagar the deaths came as pakistani police arrested more than two dozen militants from extremist groups accused of staging an attack on india parliament india has accused pakistan based lashkar taiba and jaish mohammad of carrying out the attack on december at the behest of pakistani military intelligence military tensions have soared since the raid with both sides massing troops along their border and trading tit for tat diplomatic sanctions yesterday pakistan announced it had arrested lashkar taiba chief hafiz mohammed saeed police in karachi say it is likely more raids will be launched against the two groups as well as other militant organisations accused of targetting india military tensions between india and pakistan have escalated to level not seen since their war»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the most similar document (usually the same text) is has a\n",
    "similarity score approaching 1.0. However, the similarity score for the\n",
    "second-ranked documents should be significantly lower (assuming the documents\n",
    "are in fact different) and the reasoning becomes obvious when we examine the\n",
    "text itself.\n",
    "\n",
    "We can run the next cell repeatedly to see a sampling other target-document\n",
    "comparisons.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (62): «japanese officials say their coast guard has sunk an unidentified boat after an exchange of fire in the east china sea the bbc reports the unidentified boat which resembled fishing trawler was spotted cruising off south western japan by naval reconnaissance plane more than japanese coastguard vessels were mobilised to give chase japanese officials said warning shots were fired on several occasions but the boat ignored orders to stop and continued heading west towards china the officials said crew members appeared on deck brandishing metal pipes and several hours after the chase began there was an exchange of gunfire in which two japanese sailors were injured the patrol boats then sank the vessel and its man crew was thrown into the sea some reports said it resembled north korean spy boat but there was also speculation it could have held chinese smugglers meanwhile the coast guard is continuing efforts to recover crew members six of the on board have so far been recovered from the ocean although rough weather is said to be hampering rescue efforts»\n",
      "\n",
      "Similar Document (113, 0.7472119331359863): «the immigration department says overnight fires at the woomera detention centre are part of deliberate criminal campaign by detainees it says buildings were damaged or destroyed spokesman for the department says the first fire was lit at about pm acdt last night after two earlier demonstrations about visa applications he says detainees threw rocks chairs and other objects in an effort to stop staff and local fire brigade officers from putting out the fires the department estimates the damage bill from fire related incidents at woomera has reached more than million since november last night three accommodation blocks were destroyed as well as mess hall and computing facility the department say no detainees were injured but detention centre officers were treated for smoke inhalation»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model\n",
    "-----------------\n",
    "\n",
    "Using the same approach above, we'll infer the vector for a randomly chosen\n",
    "test document, and compare the document to our model by eye.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (['Which', 'soccer', 'player', 'was', 'injured']): «Which soccer player was injured»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (96, 0.6286325454711914): «england batsman michael vaughan has become just the seventh player in the history of test cricket to be given out handling the ball vaughan was on when he flicked the ball with his hand towards fielder on the first day of the third and final test against india at bangalore the englishmen says he was disappointed in the appeal by the indians which he believes was not in the spirit of the game in the laws shouldn have done it but thought was just helping out the fielders he said feel bit disappointed on their behalf one of their players appealed not too sure who it was but bit disappointed really england finished the opening day at for mark ramprakash was out for he and vaughan adding for the fourth wicket after england was struggling at for staff at the woomera detention centre are still on red alert as result of continuing disturbances involving detainees further eight buildings were set on fire overnight and spokesman for immigration minister philip ruddock says damage from fires at the facility over the past month is now estimated at million he says six detention officers were injured during last night disturbance out of which four have returned to work he says damage to the buildings ranged from minor to extensive it is understood some detainees had helped staff extinguish fires last night the spokesman says up to detainees had tried to break through the main fence during last night disturbance and tear gas was used to prevent any escape»\n",
      "\n",
      "MEDIAN (278, 0.26618361473083496): «the royal commission looking into the collapse of insurance giant hih says the possible leak of confidential document is criminal offence royal commissioner justice neville owen has opened the public hearings into the collapse more than eight months after the company was placed into provisional liquidation in his opening statement justice owen called on all parties to adhere to the confidentiality requirements of royal commission justice owen says there could have been leak of report on the role of auditors circulated in early november it is possible that someone to whom the commission delivered copy of the report in strict confidence disclosed its contents to the author of the article if so there may have been breach of section of the royal commissions act that is criminal offence he said»\n",
      "\n",
      "LEAST (216, -0.19791565835475922): «senior taliban official confirmed the islamic militia would begin handing over its last bastion of kandahar to pashtun tribal leaders on friday this agreement was that taliban should surrender kandahar peacefully to the elders of these areas and we should guarantee the lives and the safety of taliban authorities and all the taliban from tomorrow should start this program former taliban ambassador to pakistan abdul salam zaeef told cnn in telephone interview he insisted that the taliban would not surrender to hamid karzai the new afghan interim leader and pashtun elder who has been cooperating with the united states to calm unrest among the southern tribes the taliban will surrender to elders not to karzai karzai and other persons which they want to enter kandahar by the support of america they don allow to enter kandahar city he said the taliban will surrender the weapons the ammunition to elders»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "doc_id = [\"Which\", \"soccer\", \"player\", \"was\", \"injured\"]\n",
    "#inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "inferred_vector = model.infer_vector(doc_id)\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "#print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(doc_id)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "Let's review what we've seen in this tutorial:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "That's it! Doc2Vec is a great way to explore relationships between documents.\n",
    "\n",
    "Additional Resources\n",
    "--------------------\n",
    "\n",
    "If you'd like to know more about the subject matter of this tutorial, check out the links below.\n",
    "\n",
    "* `Word2Vec Paper <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_\n",
    "* `Doc2Vec Paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`_\n",
    "* `Dr. Michael D. Lee's Website <http://faculty.sites.uci.edu/mdlee>`_\n",
    "* `Lee Corpus <http://faculty.sites.uci.edu/mdlee/similarity-data/>`__\n",
    "* `IMDB Doc2Vec Tutorial <doc2vec-IMDB.ipynb>`_\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
