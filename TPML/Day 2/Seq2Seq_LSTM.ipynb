{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvLP7JaZmAse",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "415b0f97-47ff-437d-976a-7ad7ba6c8f4e"
      },
      "source": [
        "!pip install tensorflow==2.1.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 24kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.31.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 55.8MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.18.5)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0) (49.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=c7e9f97f8aaef812864c9e8d62b24e914608541a339b3ea8963ea5bdc5bd469f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast, keras-applications, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNkx5WRymEBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras==2.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a9kxViHGv33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk import word_tokenize\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from keras.preprocessing.text import Tokenizer, one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfOWnmsBRViz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "69cd93a5-7da3-497b-b91f-22ab596edbf3"
      },
      "source": [
        "!pip install plot_keras_history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: plot_keras_history in /usr/local/lib/python3.6/dist-packages (1.1.23)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from plot_keras_history) (1.0.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from plot_keras_history) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from plot_keras_history) (1.4.1)\n",
            "Requirement already satisfied: sanitize-ml-labels in /usr/local/lib/python3.6/dist-packages (from plot_keras_history) (1.0.12)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->plot_keras_history) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->plot_keras_history) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->plot_keras_history) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot_keras_history) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot_keras_history) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->plot_keras_history) (1.2.0)\n",
            "Requirement already satisfied: compress-json in /usr/local/lib/python3.6/dist-packages (from sanitize-ml-labels->plot_keras_history) (1.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->plot_keras_history) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJGUagKRHYvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fullines= pd.read_csv('drive/My Drive/Colab Data/kopitiam.csv', header = 0)\n",
        "\n",
        "\n",
        "\n",
        "#Any pairwise and sequence data can be processed\n",
        "#Not only for translation OR Everything is \"translation\"\n",
        "#For example: \n",
        "  #Question , Answer pairs\n",
        "  #Image , caption pairs\n",
        "  #song , lyric pairs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBo654coK2g-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Preprocessing \n",
        "#Will differ according to the nature of the data. \n",
        "#This step is important!!! and time consuming!!!!\n",
        "\n",
        "#Lowercase\n",
        "fullines.SeqA=fullines.SeqA.apply(lambda x: x.lower())\n",
        "fullines.SeqB=fullines.SeqB.apply(lambda x: x.lower())\n",
        "\n",
        "#remove digits\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "fullines.SeqA=fullines.SeqA.apply(lambda x: x.translate(remove_digits))\n",
        "fullines.SeqB=fullines.SeqB.apply(lambda x: x.translate(remove_digits))\n",
        "\n",
        "#Special for Decoder SeqB\n",
        "fullines.SeqB = fullines.SeqB.apply(lambda x : '$START '+ x + ' END$')\n",
        "\n",
        "fullines.head()\n",
        "\n",
        "lines, testlines =  train_test_split(fullines, train_size = 0.8,random_state=9)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMtB_nxdRcti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f1caa845-3ee7-49a1-a302-9d82c6218c64"
      },
      "source": [
        "#tokenize and index the sequence A\n",
        "\n",
        "tokenizer_A = Tokenizer()\n",
        "tokenizer_A.fit_on_texts(lines.SeqA)\n",
        "\n",
        "seqA = tokenizer_A.texts_to_sequences(lines.SeqA)\n",
        "print(seqA[0:10])\n",
        "word_index_A = tokenizer_A.word_index\n",
        "\n",
        "vocab_size_A = len(word_index_A) + 1  # Adding 1 because of reserved 0 index by Tokenizer\n",
        "maxlen_A = max(len(x) for x in seqA) # longest text in train set\n",
        "print('vocabubary size:',vocab_size_A)\n",
        "print('max length text:',maxlen_A)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 7], [2, 4], [1, 6], [1, 7, 10, 3], [1, 7], [2, 29], [2, 6], [2, 6], [1, 4, 5]]\n",
            "vocabubary size: 63\n",
            "max length text: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT2BENUbSbOc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "a699b74a-ba58-409a-9742-cafc52eb4d34"
      },
      "source": [
        "#tokenize and index the sequence B\n",
        "\n",
        "tokenizer_B = Tokenizer(filters='!\"#%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',)\n",
        "tokenizer_B.fit_on_texts(lines.SeqB)\n",
        "\n",
        "seqB = tokenizer_B.texts_to_sequences(lines.SeqB)\n",
        "print(seqB[0:10])\n",
        "word_index_B = tokenizer_B.word_index\n",
        "\n",
        "print(word_index_B)\n",
        "\n",
        "vocab_size_B = len(word_index_B) + 1  # Adding 1 because of reserved 0 index by Tokenizer\n",
        "maxlen_B = max(len(x) for x in seqB) # longest text in train set\n",
        "print('vocabubary size:',vocab_size_B)\n",
        "print('max length text:',maxlen_B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 12, 5, 15, 6, 22, 4, 2], [1, 23, 5, 3, 9, 4, 2], [1, 7, 3, 6, 28, 2], [1, 12, 5, 3, 13, 4, 2], [1, 10, 5, 3, 9, 4, 8, 11, 5, 14, 8, 11, 6, 2], [1, 10, 5, 3, 9, 4, 8, 6, 8, 11, 5, 14, 2], [1, 7, 3, 13, 4, 2], [1, 10, 7, 3, 13, 4, 8, 6, 2], [1, 7, 3, 13, 4, 8, 6, 40, 17, 9, 4, 2], [1, 5, 15, 6, 22, 4, 2]]\n",
            "{'$start': 1, 'end$': 2, 'with': 3, 'milk': 4, 'coffee': 5, 'sugar': 6, 'tea': 7, 'and': 8, 'condensed': 9, 'hot': 10, 'more': 11, 'black': 12, 'evaporated': 13, 'powder': 14, 'without': 15, 'lesser': 16, 'of': 17, 'less': 18, 'iced': 19, 'water': 20, 'milo': 21, 'or': 22, 'strong': 23, 'but': 24, 'the': 25, 'no': 26, 'added': 27, 'only': 28, 'brew': 29, 'a': 30, 'to': 31, 'version': 32, 'initial': 33, 'ice': 34, 'weaker': 35, 'horlicks': 36, 'pulled': 37, 'is': 38, 'ginger': 39, 'instead': 40, 'they': 41, 'add': 42, 'iced\\xa0tea': 43, 'heaviest': 44, 'purest': 45, 'at': 46, 'all': 47, 'simply': 48, 'any': 49, 'kind': 50, 'weak': 51, 'thicker': 52, 'robust': 53, 'for': 54, 'heavier': 55, 'taste': 56, 'malay': 57, 'stout': 58, 'served': 59, 'teabag': 60, 'includes': 61, 'bean': 62, 'undissolved': 63, 'lemon': 64, 'chinese': 65, 'extra': 66, 'frothy': 67, 'sweet': 68, 'thinner': 69, 'dilute': 70, 'beverage': 71, 'described': 72, 'above': 73, 'cofee': 74, 'cappuccino': 75, 'concentration': 76, 'guinness': 77, 'chrysanthemum': 78, 'soya': 79, 'mixed': 80, 'grass': 81, 'jelly': 82, '‘pulled’': 83, 'specialty': 84, 'plus': 85, 'very': 86, 'lager': 87, 'guiness': 88, 'as': 89, 'topping': 90, 'home': 91, 'brewed': 92, 'chin': 93, 'chow': 94, 'soy': 95, 'pepsi': 96, 'cola': 97, 'kickapoo': 98, 'in': 99, 'bags': 100, 'thin': 101, 'takeaway': 102, 'scoop': 103, 'on': 104, 'top': 105, 'rose': 106, 'syrup': 107, 'drink': 108}\n",
            "vocabubary size: 109\n",
            "max length text: 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WofBOOxCSzFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# seqA_vec -> encoder -> seqB_vec -> decoder -> seqB_one-hot-matrix (ahead by one timestep)\n",
        "\n",
        "# First define the vectors' shape with all 0s as value\n",
        "encoder_input_data = np.zeros((len(seqA), maxlen_A), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(seqB), maxlen_B), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(seqB), maxlen_B, vocab_size_B),dtype='float32')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D7T4v9DWxfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Then populate the word_index as the value\n",
        "for i, (input_text, target_text) in enumerate(zip(seqA, seqB)):\n",
        "    for t, word_id in enumerate(input_text):\n",
        "        encoder_input_data[i, t] = word_id\n",
        "    for t, word_id in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t] = word_id\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, word_id] = 1.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLwuoSaOaVDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "8082bc73-5027-4172-ee27-7e0675bc9396"
      },
      "source": [
        "#check the index for the first pair \n",
        "print(\"encoder:\")\n",
        "print(encoder_input_data[0])\n",
        "print(decoder_input_data[0])\n",
        "print(\"target:\")\n",
        "print(decoder_target_data[0,0])\n",
        "print(decoder_target_data[0,1])\n",
        "print(decoder_target_data[0,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder:\n",
            "[1. 5. 0. 0. 0.]\n",
            "[ 1. 12.  5. 15.  6. 22.  4.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            "target:\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMfJYAVlbLLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "5d782d0a-9b01-4407-a8ba-9aa6c51da4ef"
      },
      "source": [
        "#Build Seq2Seq model using LSTM\n",
        "\n",
        "embedding_size = 64\n",
        "hidden_dim = 32\n",
        "\n",
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "\n",
        "\n",
        "#Encoder Part: {(encoder_input->embed_layer) -> lstm} -> encoder_states\n",
        "encoder_inputs = Input(shape=(None,))#define encoder_input\n",
        "en_x=  Embedding(vocab_size_A, embedding_size)(encoder_inputs)#link: encoder_input-->embed_layer\n",
        "\n",
        "encoder = LSTM(hidden_dim, return_state=True)#define lstm \n",
        "encoder_outputs, state_h, state_c = encoder(en_x)#link: (encoder_input-->embed_layer)---->lstm\n",
        "\n",
        "encoder_states = [state_h, state_c]#define the hidden/context status; reserved for decoder LSTM\n",
        "\n",
        "\n",
        "# Decoder Part:{[(decoder_input->embed_layer) , encoder_states] ----> lstm} --> last_dense_layer\n",
        "decoder_inputs = Input(shape=(None,))#define decoder_input\n",
        "\n",
        "de_x=Embedding(vocab_size_B, embedding_size)(decoder_inputs)#link:(decoder_input->embed_layer)\n",
        "\n",
        "decoder_lstm = LSTM(hidden_dim, return_sequences=True, return_state=True)#define lstm\n",
        "decoder_outputs, _, _ = decoder_lstm(de_x,initial_state=encoder_states)#link: [(decoder_input->embed_layer),encoder_states] ----> lstm\n",
        "\n",
        "decoder_dense = Dense(vocab_size_B, activation='softmax')#define last_dense_layer\n",
        "decoder_outputs = decoder_dense(decoder_outputs)#link:{[(decoder_input->embed_layer),encoder_states] ----> lstm} -> last_dense_layer\n",
        "\n",
        "#Link encoder -> decoder \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_17 (InputLayer)           (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_18 (InputLayer)           (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, None, 64)     4032        input_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_8 (Embedding)         (None, None, 64)     6976        input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   [(None, 32), (None,  12416       embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_8 (LSTM)                   [(None, None, 32), ( 12416       embedding_8[0][0]                \n",
            "                                                                 lstm_7[0][1]                     \n",
            "                                                                 lstm_7[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, None, 109)    3597        lstm_8[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 39,437\n",
            "Trainable params: 39,437\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ2Tw6fsUIC8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60e36975-ccb6-4eb6-eb29-566a38c423ee"
      },
      "source": [
        "#Train the model\n",
        "hist = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=8,\n",
        "          epochs=300,\n",
        "          validation_split=0.2).history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 107 samples, validate on 27 samples\n",
            "Epoch 1/300\n",
            "107/107 [==============================] - 1s 12ms/step - loss: 2.0406 - acc: 0.0764 - val_loss: 1.8124 - val_acc: 0.0697\n",
            "Epoch 2/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.7865 - acc: 0.0456 - val_loss: 1.4979 - val_acc: 0.0458\n",
            "Epoch 3/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.5980 - acc: 0.0704 - val_loss: 1.4156 - val_acc: 0.0937\n",
            "Epoch 4/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.5405 - acc: 0.0847 - val_loss: 1.3800 - val_acc: 0.0915\n",
            "Epoch 5/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.5081 - acc: 0.0852 - val_loss: 1.3560 - val_acc: 0.0937\n",
            "Epoch 6/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.4853 - acc: 0.0874 - val_loss: 1.3386 - val_acc: 0.0915\n",
            "Epoch 7/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 1.4615 - acc: 0.0880 - val_loss: 1.3227 - val_acc: 0.0980\n",
            "Epoch 8/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 1.4321 - acc: 0.0896 - val_loss: 1.2991 - val_acc: 0.0959\n",
            "Epoch 9/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.4034 - acc: 0.0935 - val_loss: 1.2792 - val_acc: 0.1089\n",
            "Epoch 10/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 1.3757 - acc: 0.1006 - val_loss: 1.2593 - val_acc: 0.1176\n",
            "Epoch 11/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.3492 - acc: 0.1072 - val_loss: 1.2407 - val_acc: 0.1111\n",
            "Epoch 12/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.3275 - acc: 0.1100 - val_loss: 1.2235 - val_acc: 0.1220\n",
            "Epoch 13/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 1.3046 - acc: 0.1176 - val_loss: 1.2137 - val_acc: 0.1220\n",
            "Epoch 14/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.2792 - acc: 0.1198 - val_loss: 1.1925 - val_acc: 0.1285\n",
            "Epoch 15/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 1.2604 - acc: 0.1297 - val_loss: 1.1792 - val_acc: 0.1373\n",
            "Epoch 16/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.2376 - acc: 0.1363 - val_loss: 1.1673 - val_acc: 0.1329\n",
            "Epoch 17/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 1.2141 - acc: 0.1369 - val_loss: 1.1601 - val_acc: 0.1503\n",
            "Epoch 18/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.1925 - acc: 0.1484 - val_loss: 1.1266 - val_acc: 0.1525\n",
            "Epoch 19/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.1697 - acc: 0.1550 - val_loss: 1.1122 - val_acc: 0.1634\n",
            "Epoch 20/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.1467 - acc: 0.1633 - val_loss: 1.0952 - val_acc: 0.1590\n",
            "Epoch 21/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.1231 - acc: 0.1682 - val_loss: 1.0906 - val_acc: 0.1656\n",
            "Epoch 22/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.1003 - acc: 0.1688 - val_loss: 1.0653 - val_acc: 0.1743\n",
            "Epoch 23/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.0764 - acc: 0.1781 - val_loss: 1.0525 - val_acc: 0.1808\n",
            "Epoch 24/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.0553 - acc: 0.1809 - val_loss: 1.0410 - val_acc: 0.1786\n",
            "Epoch 25/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.0319 - acc: 0.1913 - val_loss: 1.0224 - val_acc: 0.1830\n",
            "Epoch 26/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 1.0089 - acc: 0.1924 - val_loss: 1.0099 - val_acc: 0.2004\n",
            "Epoch 27/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.9899 - acc: 0.1990 - val_loss: 0.9999 - val_acc: 0.2092\n",
            "Epoch 28/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.9679 - acc: 0.2056 - val_loss: 0.9921 - val_acc: 0.2048\n",
            "Epoch 29/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.9494 - acc: 0.2095 - val_loss: 0.9767 - val_acc: 0.2092\n",
            "Epoch 30/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.9299 - acc: 0.2089 - val_loss: 0.9670 - val_acc: 0.2092\n",
            "Epoch 31/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.9128 - acc: 0.2117 - val_loss: 0.9625 - val_acc: 0.2179\n",
            "Epoch 32/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.8950 - acc: 0.2232 - val_loss: 0.9550 - val_acc: 0.2157\n",
            "Epoch 33/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.8786 - acc: 0.2232 - val_loss: 0.9483 - val_acc: 0.2157\n",
            "Epoch 34/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.8638 - acc: 0.2265 - val_loss: 0.9350 - val_acc: 0.2222\n",
            "Epoch 35/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.8483 - acc: 0.2309 - val_loss: 0.9322 - val_acc: 0.2244\n",
            "Epoch 36/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.8340 - acc: 0.2320 - val_loss: 0.9276 - val_acc: 0.2157\n",
            "Epoch 37/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.8203 - acc: 0.2342 - val_loss: 0.9238 - val_acc: 0.2244\n",
            "Epoch 38/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.8054 - acc: 0.2380 - val_loss: 0.9164 - val_acc: 0.2222\n",
            "Epoch 39/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.7921 - acc: 0.2435 - val_loss: 0.9126 - val_acc: 0.2244\n",
            "Epoch 40/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.7795 - acc: 0.2474 - val_loss: 0.9038 - val_acc: 0.2244\n",
            "Epoch 41/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.7669 - acc: 0.2507 - val_loss: 0.9040 - val_acc: 0.2200\n",
            "Epoch 42/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.7544 - acc: 0.2540 - val_loss: 0.9101 - val_acc: 0.2309\n",
            "Epoch 43/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.7424 - acc: 0.2540 - val_loss: 0.8964 - val_acc: 0.2266\n",
            "Epoch 44/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.7311 - acc: 0.2595 - val_loss: 0.8892 - val_acc: 0.2200\n",
            "Epoch 45/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.7195 - acc: 0.2589 - val_loss: 0.8981 - val_acc: 0.2418\n",
            "Epoch 46/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.7089 - acc: 0.2639 - val_loss: 0.8880 - val_acc: 0.2288\n",
            "Epoch 47/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.6983 - acc: 0.2650 - val_loss: 0.8840 - val_acc: 0.2331\n",
            "Epoch 48/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.6886 - acc: 0.2666 - val_loss: 0.8753 - val_acc: 0.2266\n",
            "Epoch 49/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.6767 - acc: 0.2661 - val_loss: 0.8811 - val_acc: 0.2309\n",
            "Epoch 50/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.6675 - acc: 0.2666 - val_loss: 0.8756 - val_acc: 0.2331\n",
            "Epoch 51/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.6576 - acc: 0.2721 - val_loss: 0.8703 - val_acc: 0.2353\n",
            "Epoch 52/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.6471 - acc: 0.2727 - val_loss: 0.8726 - val_acc: 0.2353\n",
            "Epoch 53/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.6379 - acc: 0.2754 - val_loss: 0.8635 - val_acc: 0.2309\n",
            "Epoch 54/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.6291 - acc: 0.2760 - val_loss: 0.8686 - val_acc: 0.2375\n",
            "Epoch 55/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.6188 - acc: 0.2754 - val_loss: 0.8601 - val_acc: 0.2331\n",
            "Epoch 56/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.6106 - acc: 0.2793 - val_loss: 0.8586 - val_acc: 0.2331\n",
            "Epoch 57/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.6035 - acc: 0.2749 - val_loss: 0.8536 - val_acc: 0.2353\n",
            "Epoch 58/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.5922 - acc: 0.2831 - val_loss: 0.8553 - val_acc: 0.2331\n",
            "Epoch 59/300\n",
            "107/107 [==============================] - 1s 6ms/step - loss: 0.5853 - acc: 0.2809 - val_loss: 0.8515 - val_acc: 0.2375\n",
            "Epoch 60/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.5785 - acc: 0.2826 - val_loss: 0.8523 - val_acc: 0.2331\n",
            "Epoch 61/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.5682 - acc: 0.2853 - val_loss: 0.8514 - val_acc: 0.2375\n",
            "Epoch 62/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.5603 - acc: 0.2848 - val_loss: 0.8490 - val_acc: 0.2375\n",
            "Epoch 63/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.5523 - acc: 0.2897 - val_loss: 0.8508 - val_acc: 0.2397\n",
            "Epoch 64/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.5458 - acc: 0.2908 - val_loss: 0.8456 - val_acc: 0.2418\n",
            "Epoch 65/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.5380 - acc: 0.2925 - val_loss: 0.8480 - val_acc: 0.2375\n",
            "Epoch 66/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.5292 - acc: 0.2969 - val_loss: 0.8448 - val_acc: 0.2397\n",
            "Epoch 67/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.5219 - acc: 0.2974 - val_loss: 0.8408 - val_acc: 0.2375\n",
            "Epoch 68/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.5148 - acc: 0.3029 - val_loss: 0.8394 - val_acc: 0.2397\n",
            "Epoch 69/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.5086 - acc: 0.3007 - val_loss: 0.8374 - val_acc: 0.2418\n",
            "Epoch 70/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.5008 - acc: 0.3057 - val_loss: 0.8393 - val_acc: 0.2353\n",
            "Epoch 71/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.4939 - acc: 0.3062 - val_loss: 0.8331 - val_acc: 0.2353\n",
            "Epoch 72/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.4868 - acc: 0.3084 - val_loss: 0.8345 - val_acc: 0.2375\n",
            "Epoch 73/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.4823 - acc: 0.3073 - val_loss: 0.8268 - val_acc: 0.2397\n",
            "Epoch 74/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.4741 - acc: 0.3112 - val_loss: 0.8272 - val_acc: 0.2353\n",
            "Epoch 75/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.4687 - acc: 0.3150 - val_loss: 0.8269 - val_acc: 0.2418\n",
            "Epoch 76/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.4618 - acc: 0.3156 - val_loss: 0.8281 - val_acc: 0.2397\n",
            "Epoch 77/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.4569 - acc: 0.3167 - val_loss: 0.8236 - val_acc: 0.2418\n",
            "Epoch 78/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.4495 - acc: 0.3200 - val_loss: 0.8287 - val_acc: 0.2375\n",
            "Epoch 79/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.4457 - acc: 0.3167 - val_loss: 0.8243 - val_acc: 0.2331\n",
            "Epoch 80/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.4388 - acc: 0.3172 - val_loss: 0.8255 - val_acc: 0.2353\n",
            "Epoch 81/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.4340 - acc: 0.3161 - val_loss: 0.8261 - val_acc: 0.2397\n",
            "Epoch 82/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.4270 - acc: 0.3216 - val_loss: 0.8248 - val_acc: 0.2309\n",
            "Epoch 83/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.4208 - acc: 0.3227 - val_loss: 0.8310 - val_acc: 0.2309\n",
            "Epoch 84/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.4175 - acc: 0.3227 - val_loss: 0.8245 - val_acc: 0.2288\n",
            "Epoch 85/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.4110 - acc: 0.3266 - val_loss: 0.8263 - val_acc: 0.2418\n",
            "Epoch 86/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.4048 - acc: 0.3266 - val_loss: 0.8230 - val_acc: 0.2309\n",
            "Epoch 87/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.4005 - acc: 0.3277 - val_loss: 0.8240 - val_acc: 0.2397\n",
            "Epoch 88/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.3956 - acc: 0.3315 - val_loss: 0.8231 - val_acc: 0.2418\n",
            "Epoch 89/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3901 - acc: 0.3293 - val_loss: 0.8261 - val_acc: 0.2353\n",
            "Epoch 90/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3856 - acc: 0.3326 - val_loss: 0.8247 - val_acc: 0.2353\n",
            "Epoch 91/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3800 - acc: 0.3332 - val_loss: 0.8268 - val_acc: 0.2353\n",
            "Epoch 92/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.3744 - acc: 0.3359 - val_loss: 0.8293 - val_acc: 0.2353\n",
            "Epoch 93/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3708 - acc: 0.3359 - val_loss: 0.8255 - val_acc: 0.2418\n",
            "Epoch 94/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.3649 - acc: 0.3370 - val_loss: 0.8236 - val_acc: 0.2266\n",
            "Epoch 95/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3613 - acc: 0.3386 - val_loss: 0.8218 - val_acc: 0.2375\n",
            "Epoch 96/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.3567 - acc: 0.3419 - val_loss: 0.8244 - val_acc: 0.2418\n",
            "Epoch 97/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3512 - acc: 0.3458 - val_loss: 0.8229 - val_acc: 0.2397\n",
            "Epoch 98/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3463 - acc: 0.3452 - val_loss: 0.8273 - val_acc: 0.2331\n",
            "Epoch 99/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3421 - acc: 0.3485 - val_loss: 0.8253 - val_acc: 0.2418\n",
            "Epoch 100/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.3377 - acc: 0.3485 - val_loss: 0.8346 - val_acc: 0.2331\n",
            "Epoch 101/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3336 - acc: 0.3485 - val_loss: 0.8244 - val_acc: 0.2331\n",
            "Epoch 102/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3280 - acc: 0.3518 - val_loss: 0.8274 - val_acc: 0.2418\n",
            "Epoch 103/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3239 - acc: 0.3529 - val_loss: 0.8253 - val_acc: 0.2462\n",
            "Epoch 104/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.3203 - acc: 0.3540 - val_loss: 0.8290 - val_acc: 0.2397\n",
            "Epoch 105/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3161 - acc: 0.3562 - val_loss: 0.8359 - val_acc: 0.2331\n",
            "Epoch 106/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3116 - acc: 0.3573 - val_loss: 0.8374 - val_acc: 0.2331\n",
            "Epoch 107/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3071 - acc: 0.3573 - val_loss: 0.8417 - val_acc: 0.2353\n",
            "Epoch 108/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.3043 - acc: 0.3568 - val_loss: 0.8303 - val_acc: 0.2484\n",
            "Epoch 109/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.3013 - acc: 0.3606 - val_loss: 0.8294 - val_acc: 0.2418\n",
            "Epoch 110/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.2945 - acc: 0.3639 - val_loss: 0.8323 - val_acc: 0.2397\n",
            "Epoch 111/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2909 - acc: 0.3639 - val_loss: 0.8373 - val_acc: 0.2440\n",
            "Epoch 112/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.2886 - acc: 0.3590 - val_loss: 0.8322 - val_acc: 0.2375\n",
            "Epoch 113/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2849 - acc: 0.3645 - val_loss: 0.8395 - val_acc: 0.2375\n",
            "Epoch 114/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2798 - acc: 0.3650 - val_loss: 0.8373 - val_acc: 0.2375\n",
            "Epoch 115/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.2787 - acc: 0.3650 - val_loss: 0.8351 - val_acc: 0.2397\n",
            "Epoch 116/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.2720 - acc: 0.3683 - val_loss: 0.8367 - val_acc: 0.2418\n",
            "Epoch 117/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.2694 - acc: 0.3661 - val_loss: 0.8386 - val_acc: 0.2309\n",
            "Epoch 118/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2667 - acc: 0.3689 - val_loss: 0.8461 - val_acc: 0.2418\n",
            "Epoch 119/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2610 - acc: 0.3705 - val_loss: 0.8425 - val_acc: 0.2418\n",
            "Epoch 120/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.2594 - acc: 0.3722 - val_loss: 0.8452 - val_acc: 0.2353\n",
            "Epoch 121/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2558 - acc: 0.3738 - val_loss: 0.8481 - val_acc: 0.2331\n",
            "Epoch 122/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.2519 - acc: 0.3744 - val_loss: 0.8572 - val_acc: 0.2353\n",
            "Epoch 123/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2481 - acc: 0.3771 - val_loss: 0.8589 - val_acc: 0.2353\n",
            "Epoch 124/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2454 - acc: 0.3788 - val_loss: 0.8457 - val_acc: 0.2309\n",
            "Epoch 125/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2438 - acc: 0.3766 - val_loss: 0.8472 - val_acc: 0.2353\n",
            "Epoch 126/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2386 - acc: 0.3821 - val_loss: 0.8565 - val_acc: 0.2375\n",
            "Epoch 127/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2358 - acc: 0.3810 - val_loss: 0.8497 - val_acc: 0.2353\n",
            "Epoch 128/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.2321 - acc: 0.3804 - val_loss: 0.8520 - val_acc: 0.2397\n",
            "Epoch 129/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.2299 - acc: 0.3843 - val_loss: 0.8683 - val_acc: 0.2397\n",
            "Epoch 130/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.2273 - acc: 0.3859 - val_loss: 0.8635 - val_acc: 0.2397\n",
            "Epoch 131/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2249 - acc: 0.3859 - val_loss: 0.8580 - val_acc: 0.2418\n",
            "Epoch 132/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.2204 - acc: 0.3865 - val_loss: 0.8622 - val_acc: 0.2375\n",
            "Epoch 133/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.2195 - acc: 0.3843 - val_loss: 0.8593 - val_acc: 0.2353\n",
            "Epoch 134/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2142 - acc: 0.3892 - val_loss: 0.8669 - val_acc: 0.2375\n",
            "Epoch 135/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2139 - acc: 0.3925 - val_loss: 0.8809 - val_acc: 0.2309\n",
            "Epoch 136/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.2087 - acc: 0.3920 - val_loss: 0.8632 - val_acc: 0.2353\n",
            "Epoch 137/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2074 - acc: 0.3914 - val_loss: 0.8776 - val_acc: 0.2397\n",
            "Epoch 138/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.2061 - acc: 0.3903 - val_loss: 0.8648 - val_acc: 0.2266\n",
            "Epoch 139/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.2022 - acc: 0.3920 - val_loss: 0.8740 - val_acc: 0.2244\n",
            "Epoch 140/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1996 - acc: 0.3936 - val_loss: 0.8720 - val_acc: 0.2375\n",
            "Epoch 141/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1981 - acc: 0.3969 - val_loss: 0.8818 - val_acc: 0.2353\n",
            "Epoch 142/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1943 - acc: 0.3969 - val_loss: 0.8795 - val_acc: 0.2397\n",
            "Epoch 143/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1916 - acc: 0.3942 - val_loss: 0.8795 - val_acc: 0.2375\n",
            "Epoch 144/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1898 - acc: 0.3991 - val_loss: 0.9034 - val_acc: 0.2375\n",
            "Epoch 145/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1894 - acc: 0.4019 - val_loss: 0.8765 - val_acc: 0.2397\n",
            "Epoch 146/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1840 - acc: 0.4013 - val_loss: 0.8856 - val_acc: 0.2418\n",
            "Epoch 147/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1814 - acc: 0.4019 - val_loss: 0.8748 - val_acc: 0.2375\n",
            "Epoch 148/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1804 - acc: 0.4019 - val_loss: 0.8870 - val_acc: 0.2397\n",
            "Epoch 149/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1788 - acc: 0.3986 - val_loss: 0.8809 - val_acc: 0.2353\n",
            "Epoch 150/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1753 - acc: 0.3997 - val_loss: 0.8776 - val_acc: 0.2375\n",
            "Epoch 151/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1745 - acc: 0.3997 - val_loss: 0.8896 - val_acc: 0.2418\n",
            "Epoch 152/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1707 - acc: 0.4041 - val_loss: 0.8887 - val_acc: 0.2375\n",
            "Epoch 153/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1695 - acc: 0.4013 - val_loss: 0.8785 - val_acc: 0.2527\n",
            "Epoch 154/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1682 - acc: 0.4030 - val_loss: 0.8841 - val_acc: 0.2397\n",
            "Epoch 155/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1649 - acc: 0.4035 - val_loss: 0.8912 - val_acc: 0.2440\n",
            "Epoch 156/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1631 - acc: 0.4035 - val_loss: 0.8940 - val_acc: 0.2440\n",
            "Epoch 157/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1611 - acc: 0.4041 - val_loss: 0.8906 - val_acc: 0.2418\n",
            "Epoch 158/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1599 - acc: 0.4035 - val_loss: 0.8794 - val_acc: 0.2418\n",
            "Epoch 159/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1571 - acc: 0.4030 - val_loss: 0.9029 - val_acc: 0.2440\n",
            "Epoch 160/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1539 - acc: 0.4063 - val_loss: 0.8916 - val_acc: 0.2462\n",
            "Epoch 161/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1541 - acc: 0.4046 - val_loss: 0.9005 - val_acc: 0.2440\n",
            "Epoch 162/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.1521 - acc: 0.4068 - val_loss: 0.9097 - val_acc: 0.2418\n",
            "Epoch 163/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1494 - acc: 0.4013 - val_loss: 0.9070 - val_acc: 0.2484\n",
            "Epoch 164/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1497 - acc: 0.4035 - val_loss: 0.9022 - val_acc: 0.2462\n",
            "Epoch 165/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1461 - acc: 0.4035 - val_loss: 0.9000 - val_acc: 0.2418\n",
            "Epoch 166/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.1444 - acc: 0.4074 - val_loss: 0.9043 - val_acc: 0.2462\n",
            "Epoch 167/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.1432 - acc: 0.4068 - val_loss: 0.9168 - val_acc: 0.2484\n",
            "Epoch 168/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.1403 - acc: 0.4090 - val_loss: 0.9012 - val_acc: 0.2397\n",
            "Epoch 169/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1391 - acc: 0.4063 - val_loss: 0.9141 - val_acc: 0.2375\n",
            "Epoch 170/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1365 - acc: 0.4107 - val_loss: 0.9317 - val_acc: 0.2331\n",
            "Epoch 171/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1358 - acc: 0.4079 - val_loss: 0.9122 - val_acc: 0.2440\n",
            "Epoch 172/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1344 - acc: 0.4079 - val_loss: 0.9195 - val_acc: 0.2418\n",
            "Epoch 173/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1328 - acc: 0.4068 - val_loss: 0.9291 - val_acc: 0.2440\n",
            "Epoch 174/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1321 - acc: 0.4112 - val_loss: 0.9304 - val_acc: 0.2440\n",
            "Epoch 175/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1301 - acc: 0.4052 - val_loss: 0.9203 - val_acc: 0.2331\n",
            "Epoch 176/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.1282 - acc: 0.4107 - val_loss: 0.9174 - val_acc: 0.2440\n",
            "Epoch 177/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.1269 - acc: 0.4090 - val_loss: 0.9153 - val_acc: 0.2397\n",
            "Epoch 178/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1241 - acc: 0.4112 - val_loss: 0.9252 - val_acc: 0.2418\n",
            "Epoch 179/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1245 - acc: 0.4118 - val_loss: 0.9289 - val_acc: 0.2484\n",
            "Epoch 180/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1213 - acc: 0.4118 - val_loss: 0.9294 - val_acc: 0.2418\n",
            "Epoch 181/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1203 - acc: 0.4118 - val_loss: 0.9307 - val_acc: 0.2462\n",
            "Epoch 182/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1202 - acc: 0.4129 - val_loss: 0.9410 - val_acc: 0.2397\n",
            "Epoch 183/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1186 - acc: 0.4129 - val_loss: 0.9402 - val_acc: 0.2440\n",
            "Epoch 184/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1178 - acc: 0.4123 - val_loss: 0.9396 - val_acc: 0.2397\n",
            "Epoch 185/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1133 - acc: 0.4140 - val_loss: 0.9354 - val_acc: 0.2440\n",
            "Epoch 186/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1136 - acc: 0.4173 - val_loss: 0.9353 - val_acc: 0.2331\n",
            "Epoch 187/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1137 - acc: 0.4189 - val_loss: 0.9430 - val_acc: 0.2418\n",
            "Epoch 188/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1120 - acc: 0.4162 - val_loss: 0.9624 - val_acc: 0.2353\n",
            "Epoch 189/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.1103 - acc: 0.4173 - val_loss: 0.9337 - val_acc: 0.2331\n",
            "Epoch 190/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1112 - acc: 0.4145 - val_loss: 0.9508 - val_acc: 0.2418\n",
            "Epoch 191/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1077 - acc: 0.4156 - val_loss: 0.9336 - val_acc: 0.2331\n",
            "Epoch 192/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1066 - acc: 0.4206 - val_loss: 0.9411 - val_acc: 0.2418\n",
            "Epoch 193/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.1040 - acc: 0.4184 - val_loss: 0.9414 - val_acc: 0.2397\n",
            "Epoch 194/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1057 - acc: 0.4162 - val_loss: 0.9615 - val_acc: 0.2397\n",
            "Epoch 195/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.1029 - acc: 0.4162 - val_loss: 0.9720 - val_acc: 0.2397\n",
            "Epoch 196/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.1042 - acc: 0.4189 - val_loss: 0.9452 - val_acc: 0.2375\n",
            "Epoch 197/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.1006 - acc: 0.4156 - val_loss: 0.9710 - val_acc: 0.2462\n",
            "Epoch 198/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0992 - acc: 0.4184 - val_loss: 0.9519 - val_acc: 0.2353\n",
            "Epoch 199/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.0988 - acc: 0.4189 - val_loss: 0.9610 - val_acc: 0.2353\n",
            "Epoch 200/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0982 - acc: 0.4156 - val_loss: 0.9455 - val_acc: 0.2375\n",
            "Epoch 201/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0987 - acc: 0.4173 - val_loss: 0.9617 - val_acc: 0.2331\n",
            "Epoch 202/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0958 - acc: 0.4189 - val_loss: 0.9645 - val_acc: 0.2375\n",
            "Epoch 203/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.0953 - acc: 0.4162 - val_loss: 0.9856 - val_acc: 0.2375\n",
            "Epoch 204/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0936 - acc: 0.4195 - val_loss: 0.9772 - val_acc: 0.2309\n",
            "Epoch 205/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0942 - acc: 0.4184 - val_loss: 0.9876 - val_acc: 0.2440\n",
            "Epoch 206/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0919 - acc: 0.4184 - val_loss: 0.9780 - val_acc: 0.2375\n",
            "Epoch 207/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0921 - acc: 0.4189 - val_loss: 0.9860 - val_acc: 0.2397\n",
            "Epoch 208/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0914 - acc: 0.4173 - val_loss: 0.9738 - val_acc: 0.2397\n",
            "Epoch 209/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0886 - acc: 0.4184 - val_loss: 0.9872 - val_acc: 0.2397\n",
            "Epoch 210/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0884 - acc: 0.4184 - val_loss: 0.9851 - val_acc: 0.2418\n",
            "Epoch 211/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0878 - acc: 0.4195 - val_loss: 0.9744 - val_acc: 0.2353\n",
            "Epoch 212/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0858 - acc: 0.4173 - val_loss: 0.9751 - val_acc: 0.2418\n",
            "Epoch 213/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0866 - acc: 0.4167 - val_loss: 0.9968 - val_acc: 0.2375\n",
            "Epoch 214/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0859 - acc: 0.4211 - val_loss: 0.9847 - val_acc: 0.2331\n",
            "Epoch 215/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0841 - acc: 0.4184 - val_loss: 0.9919 - val_acc: 0.2353\n",
            "Epoch 216/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0837 - acc: 0.4184 - val_loss: 0.9963 - val_acc: 0.2309\n",
            "Epoch 217/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0842 - acc: 0.4195 - val_loss: 0.9888 - val_acc: 0.2418\n",
            "Epoch 218/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0827 - acc: 0.4206 - val_loss: 0.9964 - val_acc: 0.2353\n",
            "Epoch 219/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0812 - acc: 0.4184 - val_loss: 0.9916 - val_acc: 0.2418\n",
            "Epoch 220/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0801 - acc: 0.4195 - val_loss: 1.0018 - val_acc: 0.2375\n",
            "Epoch 221/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0811 - acc: 0.4178 - val_loss: 1.0201 - val_acc: 0.2462\n",
            "Epoch 222/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0790 - acc: 0.4211 - val_loss: 1.0024 - val_acc: 0.2418\n",
            "Epoch 223/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0781 - acc: 0.4206 - val_loss: 1.0132 - val_acc: 0.2397\n",
            "Epoch 224/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0777 - acc: 0.4217 - val_loss: 1.0098 - val_acc: 0.2375\n",
            "Epoch 225/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0762 - acc: 0.4222 - val_loss: 1.0113 - val_acc: 0.2375\n",
            "Epoch 226/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0765 - acc: 0.4206 - val_loss: 1.0033 - val_acc: 0.2418\n",
            "Epoch 227/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0756 - acc: 0.4200 - val_loss: 1.0176 - val_acc: 0.2440\n",
            "Epoch 228/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.0747 - acc: 0.4228 - val_loss: 1.0084 - val_acc: 0.2397\n",
            "Epoch 229/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0733 - acc: 0.4200 - val_loss: 1.0199 - val_acc: 0.2375\n",
            "Epoch 230/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0734 - acc: 0.4206 - val_loss: 1.0188 - val_acc: 0.2397\n",
            "Epoch 231/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0747 - acc: 0.4189 - val_loss: 1.0121 - val_acc: 0.2418\n",
            "Epoch 232/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0722 - acc: 0.4200 - val_loss: 1.0088 - val_acc: 0.2397\n",
            "Epoch 233/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0716 - acc: 0.4228 - val_loss: 1.0423 - val_acc: 0.2397\n",
            "Epoch 234/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0730 - acc: 0.4206 - val_loss: 1.0143 - val_acc: 0.2375\n",
            "Epoch 235/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0700 - acc: 0.4217 - val_loss: 1.0249 - val_acc: 0.2397\n",
            "Epoch 236/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0696 - acc: 0.4217 - val_loss: 1.0308 - val_acc: 0.2331\n",
            "Epoch 237/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0699 - acc: 0.4206 - val_loss: 1.0317 - val_acc: 0.2375\n",
            "Epoch 238/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0678 - acc: 0.4228 - val_loss: 1.0288 - val_acc: 0.2397\n",
            "Epoch 239/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0688 - acc: 0.4200 - val_loss: 1.0195 - val_acc: 0.2331\n",
            "Epoch 240/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0678 - acc: 0.4173 - val_loss: 1.0308 - val_acc: 0.2375\n",
            "Epoch 241/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0672 - acc: 0.4211 - val_loss: 1.0341 - val_acc: 0.2418\n",
            "Epoch 242/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0675 - acc: 0.4195 - val_loss: 1.0391 - val_acc: 0.2331\n",
            "Epoch 243/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0667 - acc: 0.4206 - val_loss: 1.0374 - val_acc: 0.2353\n",
            "Epoch 244/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0649 - acc: 0.4206 - val_loss: 1.0376 - val_acc: 0.2331\n",
            "Epoch 245/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0652 - acc: 0.4189 - val_loss: 1.0399 - val_acc: 0.2375\n",
            "Epoch 246/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0652 - acc: 0.4222 - val_loss: 1.0277 - val_acc: 0.2309\n",
            "Epoch 247/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0636 - acc: 0.4233 - val_loss: 1.0534 - val_acc: 0.2353\n",
            "Epoch 248/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0631 - acc: 0.4233 - val_loss: 1.0469 - val_acc: 0.2331\n",
            "Epoch 249/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.0635 - acc: 0.4211 - val_loss: 1.0479 - val_acc: 0.2440\n",
            "Epoch 250/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.0627 - acc: 0.4206 - val_loss: 1.0531 - val_acc: 0.2375\n",
            "Epoch 251/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0619 - acc: 0.4244 - val_loss: 1.0515 - val_acc: 0.2353\n",
            "Epoch 252/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0612 - acc: 0.4217 - val_loss: 1.0521 - val_acc: 0.2288\n",
            "Epoch 253/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0622 - acc: 0.4195 - val_loss: 1.0528 - val_acc: 0.2331\n",
            "Epoch 254/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0614 - acc: 0.4206 - val_loss: 1.0497 - val_acc: 0.2375\n",
            "Epoch 255/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0615 - acc: 0.4200 - val_loss: 1.0586 - val_acc: 0.2397\n",
            "Epoch 256/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0607 - acc: 0.4233 - val_loss: 1.0465 - val_acc: 0.2375\n",
            "Epoch 257/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0608 - acc: 0.4222 - val_loss: 1.0581 - val_acc: 0.2331\n",
            "Epoch 258/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0586 - acc: 0.4222 - val_loss: 1.0611 - val_acc: 0.2331\n",
            "Epoch 259/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0585 - acc: 0.4239 - val_loss: 1.0696 - val_acc: 0.2397\n",
            "Epoch 260/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0584 - acc: 0.4211 - val_loss: 1.0611 - val_acc: 0.2397\n",
            "Epoch 261/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.0580 - acc: 0.4217 - val_loss: 1.0740 - val_acc: 0.2266\n",
            "Epoch 262/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0572 - acc: 0.4239 - val_loss: 1.0895 - val_acc: 0.2309\n",
            "Epoch 263/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0598 - acc: 0.4217 - val_loss: 1.0774 - val_acc: 0.2353\n",
            "Epoch 264/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0569 - acc: 0.4244 - val_loss: 1.0733 - val_acc: 0.2375\n",
            "Epoch 265/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0573 - acc: 0.4217 - val_loss: 1.0806 - val_acc: 0.2375\n",
            "Epoch 266/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0558 - acc: 0.4233 - val_loss: 1.0704 - val_acc: 0.2397\n",
            "Epoch 267/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0562 - acc: 0.4222 - val_loss: 1.0829 - val_acc: 0.2375\n",
            "Epoch 268/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0556 - acc: 0.4217 - val_loss: 1.0769 - val_acc: 0.2375\n",
            "Epoch 269/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0550 - acc: 0.4222 - val_loss: 1.0812 - val_acc: 0.2353\n",
            "Epoch 270/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.0551 - acc: 0.4255 - val_loss: 1.0742 - val_acc: 0.2288\n",
            "Epoch 271/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0535 - acc: 0.4239 - val_loss: 1.0798 - val_acc: 0.2397\n",
            "Epoch 272/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0540 - acc: 0.4233 - val_loss: 1.0757 - val_acc: 0.2331\n",
            "Epoch 273/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.0541 - acc: 0.4228 - val_loss: 1.0749 - val_acc: 0.2331\n",
            "Epoch 274/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0532 - acc: 0.4239 - val_loss: 1.0836 - val_acc: 0.2288\n",
            "Epoch 275/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0529 - acc: 0.4239 - val_loss: 1.0849 - val_acc: 0.2375\n",
            "Epoch 276/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.0527 - acc: 0.4239 - val_loss: 1.0878 - val_acc: 0.2375\n",
            "Epoch 277/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0528 - acc: 0.4239 - val_loss: 1.0841 - val_acc: 0.2375\n",
            "Epoch 278/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0514 - acc: 0.4244 - val_loss: 1.0949 - val_acc: 0.2288\n",
            "Epoch 279/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0548 - acc: 0.4222 - val_loss: 1.0917 - val_acc: 0.2375\n",
            "Epoch 280/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0509 - acc: 0.4255 - val_loss: 1.0904 - val_acc: 0.2309\n",
            "Epoch 281/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0516 - acc: 0.4233 - val_loss: 1.0898 - val_acc: 0.2331\n",
            "Epoch 282/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0499 - acc: 0.4239 - val_loss: 1.1055 - val_acc: 0.2353\n",
            "Epoch 283/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0520 - acc: 0.4244 - val_loss: 1.0973 - val_acc: 0.2331\n",
            "Epoch 284/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0516 - acc: 0.4239 - val_loss: 1.1072 - val_acc: 0.2353\n",
            "Epoch 285/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0508 - acc: 0.4217 - val_loss: 1.0937 - val_acc: 0.2353\n",
            "Epoch 286/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.0488 - acc: 0.4222 - val_loss: 1.1032 - val_acc: 0.2375\n",
            "Epoch 287/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0497 - acc: 0.4244 - val_loss: 1.1036 - val_acc: 0.2331\n",
            "Epoch 288/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0495 - acc: 0.4222 - val_loss: 1.0919 - val_acc: 0.2397\n",
            "Epoch 289/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0481 - acc: 0.4222 - val_loss: 1.1004 - val_acc: 0.2331\n",
            "Epoch 290/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0479 - acc: 0.4244 - val_loss: 1.1069 - val_acc: 0.2331\n",
            "Epoch 291/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0488 - acc: 0.4233 - val_loss: 1.1003 - val_acc: 0.2418\n",
            "Epoch 292/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0480 - acc: 0.4239 - val_loss: 1.1058 - val_acc: 0.2331\n",
            "Epoch 293/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0485 - acc: 0.4244 - val_loss: 1.1031 - val_acc: 0.2397\n",
            "Epoch 294/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0474 - acc: 0.4228 - val_loss: 1.1111 - val_acc: 0.2375\n",
            "Epoch 295/300\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.0502 - acc: 0.4195 - val_loss: 1.0975 - val_acc: 0.2353\n",
            "Epoch 296/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0463 - acc: 0.4261 - val_loss: 1.1135 - val_acc: 0.2397\n",
            "Epoch 297/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0466 - acc: 0.4233 - val_loss: 1.1089 - val_acc: 0.2375\n",
            "Epoch 298/300\n",
            "107/107 [==============================] - 0s 5ms/step - loss: 0.0471 - acc: 0.4211 - val_loss: 1.1275 - val_acc: 0.2353\n",
            "Epoch 299/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0456 - acc: 0.4239 - val_loss: 1.1180 - val_acc: 0.2375\n",
            "Epoch 300/300\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 0.0468 - acc: 0.4206 - val_loss: 1.1055 - val_acc: 0.2375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7CLju35P3oK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "3f772b9f-94f8-44b9-fa37-c518823b3f29"
      },
      "source": [
        "\n",
        "from plot_keras_history import plot_history\n",
        "plot_history(hist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsz6McDxix8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We are ready for inference. \n",
        "#(kopi o <pad> <pad> <pad>)\t---------> ($START black coffee with sugar END$)\n",
        "  ##Encoder(kopi o <pad> <pad> <pad>) = (h0,c0)\n",
        "    ##(h0,c0) +  $START ----decoder-----> (black,  (h1,c1)) \n",
        "    ##(h1,c1)\t+  black -----decoder---->  (coffee, (h2,c2))\n",
        "    ##(h2,c2)\t+  coffee ----decoder-----> (with,   (h3,c3))\n",
        "    ##(h3,c3)\t+  with ------decoder--->   (sugar,  (h4,c4))\n",
        "    ##(h4,c4)\t+  sugar -----decoder---->  (END$,   (h5,c5))\n",
        "\n",
        "\n",
        "#To decode a test sentence, we will repeatedly:\n",
        "#1) Encode the input sentence and retrieve the initial state (encoder_states = [state_h, state_c])\n",
        "        ##(kopi o)--->encoder_states = [state_h, state_c]\n",
        "#2) Run one step of the decoder ([state_h, state_c] + START_ -----> black)\n",
        "    #initial state(encoder_states = [state_h, state_c]) and a \"START_\" token as input. \n",
        "    #The output will be the next Target_Word (may or maynot be \"black\").\n",
        "#3) Append the Target_Word with previous input (\"START_ Target_Word\")  and REPEAT (utill \"_END\" predicted)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCk7N-le65Lp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "outputId": "716371c7-7906-4379-db09-39c862c535ab"
      },
      "source": [
        "#define encode_model seperatly as training stage \n",
        "#(kopi o)--->encoder_states = [state_h, state_c]\n",
        "encoder_model = Model(encoder_inputs, encoder_states) #reusing the [encoder_inputs,encoder_states]\n",
        "encoder_model.summary()\n",
        "\n",
        "#define decoder_model seperatly as training stages\n",
        "#[_h, _c] for decoder LSTM\n",
        "decoder_state_input_h = Input(shape=(hidden_dim,))\n",
        "decoder_state_input_c = Input(shape=(hidden_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# [(decoder_input->embed_layer), h_t0, c_t0] for decoder LSTM\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(de_x, initial_state=decoder_states_inputs)#reusing the decoder_lstm and de_x\n",
        "# predict [h_t1, c_t1]\n",
        "decoder_states = [state_h, state_c]\n",
        "# predict [target_Seq_t1]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "decoder_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_17 (InputLayer)        (None, None)              0         \n",
            "_________________________________________________________________\n",
            "embedding_7 (Embedding)      (None, None, 64)          4032      \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                [(None, 32), (None, 32),  12416     \n",
            "=================================================================\n",
            "Total params: 16,448\n",
            "Trainable params: 16,448\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_18 (InputLayer)           (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_8 (Embedding)         (None, None, 64)     6976        input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_19 (InputLayer)           (None, 32)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_20 (InputLayer)           (None, 32)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_8 (LSTM)                   [(None, None, 32), ( 12416       embedding_8[0][0]                \n",
            "                                                                 input_19[0][0]                   \n",
            "                                                                 input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, None, 109)    3597        lstm_8[1][0]                     \n",
            "==================================================================================================\n",
            "Total params: 22,989\n",
            "Trainable params: 22,989\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SUHvsHC67mN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " ##Encoder(kopi o <pad> <pad> <pad>) = (h0,c0)\n",
        "    ##(h0,c0) +  $START ----decoder-----> (black,  (h1,c1)) \n",
        "    ##(h1,c1)\t+  black -----decoder---->  (coffee, (h2,c2))\n",
        "    ##(h2,c2)\t+  coffee ----decoder-----> (with,   (h3,c3))\n",
        "    ##(h3,c3)\t+  with ------decoder--->   (sugar,  (h4,c4))\n",
        "    ##(h4,c4)\t+  sugar -----decoder---->  (END$,   (h5,c5))\n",
        "\n",
        "def decode_sequence(input_seq,num_decoder_tokens,encoder_model,decoder_model,vocab_B,max_decoder_seq_length):\n",
        "\n",
        "    if len(input_seq)==0:\n",
        "        return [vocab_B['end$']]\n",
        "    # Encode the input as state vectors.\n",
        "    ##Encoder(kopi o <pad> <pad> <pad>) = (h0,c0)\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of word_token.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Initialize with $start\n",
        "    target_seq[0, 0] = vocab_B['$start']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1 (greedy decoding)).\n",
        "    stop_condition = False\n",
        "    decoded_word_index = []\n",
        "    \n",
        "    while not stop_condition:\n",
        "        ##(h0,c0) +  $START ----decoder-----> (???,  (h1,c1)) \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Predict the best token (black)\n",
        "        predict_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        decoded_word_index.append(predict_token_index)\n",
        "\n",
        "        # Exit condition: either hit max length # or find stop character.\n",
        "        if (predict_token_index == vocab_B['end$'] or\n",
        "           len(decoded_word_index) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        ##(h1,c1)\t+  black -----decoder---->  (???, (h2,c2))\n",
        "\n",
        "        # Update the target sequence to the predict word_token\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = predict_token_index\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrHlpODYk1_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq,num_decoder_tokens,encoder_model,decoder_model,vocab_B,max_decoder_seq_length):\n",
        "\n",
        "    if len(input_seq)==0:\n",
        "        return [vocab_B['end$']]\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of word_token.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Initialize with $start\n",
        "    target_seq[0, 0] = vocab_B['$start']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1 (greedy decoding)).\n",
        "    stop_condition = False\n",
        "    decoded_word_index = []\n",
        "    \n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Predict a token\n",
        "        predict_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        decoded_word_index.append(predict_token_index)\n",
        "\n",
        "        # Exit condition: either hit max length # or find stop character.\n",
        "        if (predict_token_index == vocab_B['end$'] or\n",
        "           len(decoded_word_index) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence to the predict word_token\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = predict_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHsBrNmnAjN_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ff2d0a7f-8c05-4855-881e-b748f6050d0e"
      },
      "source": [
        "#Try with the first row\n",
        "\n",
        "print(seqA[0])\n",
        "r = decode_sequence(seqA[0],vocab_size_B,encoder_model,decoder_model,word_index_B,maxlen_B)\n",
        "print (r)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 5]\n",
            "[19, 12, 5, 14, 8, 11, 6, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h47IfppdVbCt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b88624da-7f91-4a08-b296-cfc104329be8"
      },
      "source": [
        "# Creating a reverse dictionary\n",
        "\n",
        "reverse_word_map_A = dict(map(reversed, tokenizer_A.word_index.items()))\n",
        "reverse_word_map_B = dict(map(reversed, tokenizer_B.word_index.items()))\n",
        "\n",
        "# Function takes a tokenized sentence and returns the words\n",
        "def indexSeq_to_text_A(list_of_indices):\n",
        "    # Looking up words in dictionary\n",
        "    words = [reverse_word_map_A.get(letter) for letter in list_of_indices]\n",
        "    return(words)\n",
        "\n",
        "def indexSeq_to_text_B(list_of_indices):\n",
        "    # Looking up words in dictionary\n",
        "    words = [reverse_word_map_B.get(letter) for letter in list_of_indices]\n",
        "    return(words)\n",
        "\n",
        "print(indexSeq_to_text_A(seqA[0]))\n",
        "print(indexSeq_to_text_B(r))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['kopi', 'kosong']\n",
            "['iced', 'black', 'coffee', 'powder', 'and', 'more', 'sugar', 'end$']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03e1PEIfj2AM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Translater for a list of seqA to seqB     \n",
        "def testSeq2Sq(listOfSeqA):\n",
        "    listOfSeqB=[]\n",
        "    token_seqA = tokenizer_A.texts_to_sequences(listOfSeqA)\n",
        "    \n",
        "    for a in token_seqA:\n",
        "        r = decode_sequence(a,vocab_size_B,encoder_model,decoder_model,word_index_B,maxlen_B)\n",
        "        tokens_b = indexSeq_to_text_B(r)\n",
        "        sentB = ' '.join(tokens_b)\n",
        "        listOfSeqB.append(sentB)\n",
        "        \n",
        "    return listOfSeqB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeDvkEVFxdVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "0b2fe978-cc13-423c-c5cf-29829850c7a7"
      },
      "source": [
        "#test with fresh data\n",
        "\n",
        "decode_testSeqB = testSeq2Sq(testlines.SeqA)\n",
        "testlines['Translated'] = decode_testSeqB\n",
        "testlines.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SeqA</th>\n",
              "      <th>SeqB</th>\n",
              "      <th>Translated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>kopi gau peng</td>\n",
              "      <td>$START strong iced coffee with condensed milk ...</td>\n",
              "      <td>iced black coffee powder and more sugar end$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>teh c</td>\n",
              "      <td>$START tea with evaporated milk and sugar END$</td>\n",
              "      <td>iced milk black coffee with sugar end$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>diao yu</td>\n",
              "      <td>$START chinese tea END$</td>\n",
              "      <td>chinese tea end$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>kopi gau</td>\n",
              "      <td>$START strong brew of coffee with condensed mi...</td>\n",
              "      <td>iced black coffee powder and more sugar end$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>kopi o gau siew dai</td>\n",
              "      <td>$START hot black coffee with more coffee powde...</td>\n",
              "      <td>iced black coffee powder and more sugar end$</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    SeqA  ...                                    Translated\n",
              "13         kopi gau peng  ...  iced black coffee powder and more sugar end$\n",
              "159                teh c  ...        iced milk black coffee with sugar end$\n",
              "69               diao yu  ...                              chinese tea end$\n",
              "148             kopi gau  ...  iced black coffee powder and more sugar end$\n",
              "44   kopi o gau siew dai  ...  iced black coffee powder and more sugar end$\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCTXYqFZkGNK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "c7b624c9-b7f5-45cb-a514-4e9b356e3510"
      },
      "source": [
        "#test with known training data\n",
        "sample_train=lines[:10]\n",
        "decode_sample_train = testSeq2Sq(sample_train.SeqA)\n",
        "sample_train['Translated'] = decode_sample_train\n",
        "sample_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SeqA</th>\n",
              "      <th>SeqB</th>\n",
              "      <th>Translated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>kopi kosong</td>\n",
              "      <td>$START black coffee without sugar or milk END$</td>\n",
              "      <td>iced black coffee powder and more sugar end$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>kopi gau</td>\n",
              "      <td>$START strong coffee with condensed milk END$</td>\n",
              "      <td>iced black coffee powder and more sugar end$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>teh o</td>\n",
              "      <td>$START tea with sugar only END$</td>\n",
              "      <td>iced milk black coffee with sugar end$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kopi c</td>\n",
              "      <td>$START black coffee with evaporated milk END$</td>\n",
              "      <td>iced black coffee powder and more sugar end$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>kopi gau ga dai</td>\n",
              "      <td>$START hot coffee with condensed milk and more...</td>\n",
              "      <td>iced black coffee powder and more sugar end$</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                SeqA  ...                                    Translated\n",
              "3        kopi kosong  ...  iced black coffee powder and more sugar end$\n",
              "9           kopi gau  ...  iced black coffee powder and more sugar end$\n",
              "161            teh o  ...        iced milk black coffee with sugar end$\n",
              "2             kopi c  ...  iced black coffee powder and more sugar end$\n",
              "25   kopi gau ga dai  ...  iced black coffee powder and more sugar end$\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    }
  ]
}