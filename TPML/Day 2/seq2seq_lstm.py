# -*- coding: utf-8 -*-
"""Seq2Seq_LSTM.ipynb

Automatically generated by Colaboratory. But Not Working on Colab

Original file is located at
    https://colab.research.google.com/drive/1qvwvDXWP28VfVU6kQ8CF_RHSAKCSNzB0
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import string
from string import digits
import matplotlib.pyplot as plt
# %matplotlib inline
import re
from sklearn.model_selection import train_test_split
from nltk import word_tokenize
from gensim.corpora.dictionary import Dictionary
from keras.preprocessing.text import Tokenizer, one_hot


fullines= pd.read_csv('kopitiam.csv', header = 0)
fullines[0:10]

#Any pairwise and sequence data can be processed
#Not only for translation OR Everything is "translation"
#For example: 
  #Question , Answer pairs
  #Image , caption pairs
  #song , lyric pairs

#Data Preprocessing 
#Will differ according to the nature of the data. 
#This step is important!!! and time consuming!!!!

#Lowercase
fullines.SeqA=fullines.SeqA.apply(lambda x: x.lower())
fullines.SeqB=fullines.SeqB.apply(lambda x: x.lower())

#remove digits
remove_digits = str.maketrans('', '', digits)
fullines.SeqA=fullines.SeqA.apply(lambda x: x.translate(remove_digits))
fullines.SeqB=fullines.SeqB.apply(lambda x: x.translate(remove_digits))

#Special for Decoder SeqB
fullines.SeqB = fullines.SeqB.apply(lambda x : '$START '+ x + ' END$')

fullines.head()

lines, testlines =  train_test_split(fullines, train_size = 0.8,random_state=9)


#tokenize and index the sequence A

tokenizer_A = Tokenizer()
tokenizer_A.fit_on_texts(lines.SeqA)

seqA = tokenizer_A.texts_to_sequences(lines.SeqA)
print(seqA[0:10])
word_index_A = tokenizer_A.word_index

vocab_size_A = len(word_index_A) + 1  # Adding 1 because of reserved 0 index by Tokenizer
maxlen_A = max(len(x) for x in seqA) # longest text in train set
print('vocabubary size:',vocab_size_A)
print('max length text:',maxlen_A)

#tokenize and index the sequence B

tokenizer_B = Tokenizer(filters='!"#%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',)
tokenizer_B.fit_on_texts(lines.SeqB)

seqB = tokenizer_B.texts_to_sequences(lines.SeqB)
print(seqB[0:10])
word_index_B = tokenizer_B.word_index

print(word_index_B)

vocab_size_B = len(word_index_B) + 1  # Adding 1 because of reserved 0 index by Tokenizer
maxlen_B = max(len(x) for x in seqB) # longest text in train set
print('vocabubary size:',vocab_size_B)
print('max length text:',maxlen_B)

# seqA_vec -> encoder -> seqB_vec -> decoder -> seqB_one-hot-matrix (ahead by one timestep)

# First define the vectors' shape with all 0s as value
encoder_input_data = np.zeros((len(seqA), maxlen_A), dtype='float32')
decoder_input_data = np.zeros((len(seqB), maxlen_B), dtype='float32')
decoder_target_data = np.zeros((len(seqB), maxlen_B, vocab_size_B),dtype='float32')

#Then populate the word_index as the value
for i, (input_text, target_text) in enumerate(zip(seqA, seqB)):
    for t, word_id in enumerate(input_text):
        encoder_input_data[i, t] = word_id
    for t, word_id in enumerate(target_text):
        # decoder_target_data is ahead of decoder_input_data by one timestep
        decoder_input_data[i, t] = word_id
        if t > 0:
            # decoder_target_data will be ahead by one timestep
            # and will not include the start character.
            decoder_target_data[i, t - 1, word_id] = 1.0

#check the index for the first pair 
print("encoder:")
print(encoder_input_data[0])
print(decoder_input_data[0])
print("target:")
print(decoder_target_data[0,0])
print(decoder_target_data[0,1])
print(decoder_target_data[0,2])

#Build Seq2Seq model using LSTM

embedding_size = 64
hidden_dim = 32

from keras.layers import Input, LSTM, Embedding, Dense
from keras.models import Model
from keras.utils import plot_model


#Encoder Part: {(encoder_input->embed_layer) -> lstm} -> encoder_states
encoder_inputs = Input(shape=(None,))#define encoder_input
en_x=  Embedding(vocab_size_A, embedding_size)(encoder_inputs)#link: encoder_input-->embed_layer

encoder = LSTM(hidden_dim, return_state=True)#define lstm 
encoder_outputs, state_h, state_c = encoder(en_x)#link: (encoder_input-->embed_layer)---->lstm

encoder_states = [state_h, state_c]#define the hidden/context status; reserved for decoder LSTM


# Decoder Part:{[(decoder_input->embed_layer) , encoder_states] ----> lstm} --> last_dense_layer
decoder_inputs = Input(shape=(None,))#define decoder_input

de_x=Embedding(vocab_size_B, embedding_size)(decoder_inputs)#link:(decoder_input->embed_layer)

decoder_lstm = LSTM(hidden_dim, return_sequences=True, return_state=True)#define lstm
decoder_outputs, _, _ = decoder_lstm(de_x,initial_state=encoder_states)#link: [(decoder_input->embed_layer),encoder_states] ----> lstm

decoder_dense = Dense(vocab_size_B, activation='softmax')#define last_dense_layer
decoder_outputs = decoder_dense(decoder_outputs)#link:{[(decoder_input->embed_layer),encoder_states] ----> lstm} -> last_dense_layer

#Link encoder -> decoder 
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])

model.summary()

#Train the model
hist = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
          batch_size=16,
          epochs=150,
          validation_split=0.2).history

from plot_keras_history import plot_history
plot_history(hist)

#We are ready for inference. 
#(kopi o <pad> <pad> <pad>)	---------> ($START black coffee with sugar END$)
  ##Encoder(kopi o <pad> <pad> <pad>) = (h0,c0)
    ##(h0,c0)   +  $START ----decoder-----> (black,  (h1,c1)) 
    ##(h1,c1)	+  black -----decoder---->  (coffee, (h2,c2))
    ##(h2,c2)	+  coffee ----decoder-----> (with,   (h3,c3))
    ##(h3,c3)	+  with ------decoder--->   (sugar,  (h4,c4))
    ##(h4,c4)	+  sugar -----decoder---->  (END$,   (h5,c5))


#To decode a test sentence, we will repeatedly:
#1) Encode the input sentence and retrieve the initial state (encoder_states = [state_h, state_c])
        ##(kopi o)--->encoder_states = [state_h, state_c]
#2) Run one step of the decoder ([state_h, state_c] + START_ -----> black)
    #initial state(encoder_states = [state_h, state_c]) and a "START_" token as input. 
    #The output will be the next Target_Word (may or maynot be "black").
#3) Append the Target_Word with previous input ("START_ Target_Word")  and REPEAT (utill "_END" predicted).

#define encode_model seperatly as training stage 
#(kopi o)--->encoder_states = [state_h, state_c]
encoder_model = Model(encoder_inputs, encoder_states)
encoder_model.summary()

#define decoder_model seperatly as training stages
decoder_state_input_h = Input(shape=(hidden_dim,))
decoder_state_input_c = Input(shape=(hidden_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]


decoder_outputs, state_h, state_c = decoder_lstm(de_x, initial_state=decoder_states_inputs)
decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)

decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)
decoder_model.summary()

def decode_sequence(input_seq,num_decoder_tokens,encoder_model,decoder_model,vocab_B,max_decoder_seq_length):

    if len(input_seq)==0:
        return [vocab_B['end$']]
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq)

    # Generate empty target sequence of word_token.
    target_seq = np.zeros((1, 1))
    # Initialize with $start
    target_seq[0, 0] = vocab_B['$start']

    # Sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1 (greedy decoding)).
    stop_condition = False
    decoded_word_index = []
    
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

        # Predict a token
        predict_token_index = np.argmax(output_tokens[0, -1, :])
        decoded_word_index.append(predict_token_index)

        # Exit condition: either hit max length # or find stop character.
        if (predict_token_index == vocab_B['end$'] or
           len(decoded_word_index) > max_decoder_seq_length):
            stop_condition = True

        # Update the target sequence to the predict word_token
        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = predict_token_index

        # Update states
        states_value = [h, c]

    return decoded_word_index


print(seqA[0])

r = decode_sequence(seqA[0],vocab_size_B,encoder_model,decoder_model,word_index_B,maxlen_B)

print (r)

# Creating a reverse dictionary

reverse_word_map_A = dict(map(reversed, tokenizer_A.word_index.items()))
reverse_word_map_B = dict(map(reversed, tokenizer_B.word_index.items()))

# Function takes a tokenized sentence and returns the words
def indexSeq_to_text_A(list_of_indices):
    # Looking up words in dictionary
    words = [reverse_word_map_A.get(letter) for letter in list_of_indices]
    return(words)

def indexSeq_to_text_B(list_of_indices):
    # Looking up words in dictionary
    words = [reverse_word_map_B.get(letter) for letter in list_of_indices]
    return(words)

print(indexSeq_to_text_A(seqA[0]))
print(indexSeq_to_text_B(r))


    
def testSeq2Sq(listOfSeqA):
    
    listOfSeqB=[]
    token_seqA = tokenizer_A.texts_to_sequences(listOfSeqA)
    
    for a in token_seqA:
        r = decode_sequence(a,vocab_size_B,encoder_model,decoder_model,word_index_B,maxlen_B)
        tokens_b = indexSeq_to_text_B(r)
        sentB = ' '.join(tokens_b)
        listOfSeqB.append(sentB)
        
    return listOfSeqB

decode_testSeqB = testSeq2Sq(testlines.SeqA)
testlines['Translated'] = decode_testSeqB
testlines.head()

sample_train=lines[:10]
decode_sample_train = testSeq2Sq(sample_train.SeqA)
sample_train['Translated'] = decode_sample_train


